{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 load their model and use it, also inspect its weights shapes\n",
    "# step 2 replicate their model with our modules, load their weights into our model and replicate trivial results\n",
    "# step 3 set up loss and check it, init, small performance improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - weights scaling wrt num of residual layers: apply scaling of 1/sqrt(Num of residual layers)\n",
    "# - autocast mixed precision\n",
    "# - flash attention\n",
    "# - remove all numbers that are not clean powers of 2\n",
    "# - adamW betas, fusion, wds\n",
    "# - clip_grad_norm\n",
    "# - ad hoc param wd\n",
    "# - normlayer even b4 last lin layer\n",
    "# - torch.compile\n",
    "# - weights init\n",
    "# - in mlps after attention do 4x upsapling in hidden layer\n",
    "# - if possible do weight tying\n",
    "# - configure optimizer wd for each param\n",
    "# - lr decay: idea: durining beginning of training small batch size (that is increased l8r during training) and an high lr is good cuz very easy gains in\n",
    "#       driving down eg biases of very rare tokens, so in the beginning speed up training by boosting lr and then drive it down\n",
    "# - gradient accumulation to simulate larger bs\n",
    "# - DistributeDataParallel:\n",
    "#       CARE: this require quite a major refactoring cuz the same script is executed by different processes each one with its own associated gpu\n",
    "#       it spawns #numgpus processes\n",
    "#       each work on numgpus different chunks of our data\n",
    "#       at application of grad to weights it applies the avg of the numgpus different contributions\n",
    "#\n",
    "#       to launch it we need to use the cmd torchrun which will launch 8 different processes of the python script ur launching\n",
    "#       so 8 python interpreters will read the python script ur launching\n",
    "#\n",
    "#       torchrun --standalone --nproc_per_node=8 train.py\n",
    "#       it does: loss.backward() into gradient syncronization for averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "# set up DDP: Distributed Data Parallel\n",
    "# torch run command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
    "\n",
    "# We can modify os.environ but any changes will be effective only for the current process where it was assigned and it will not change the value permanently.\n",
    "\n",
    "is_ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "\n",
    "if is_ddp:\n",
    "    assert torch.cuda.is_available(), 'cuda is required for ddp'\n",
    "    init_process_group(backend='nccl') # initialize processes pool, when ending pool u must call destroy_process_group\n",
    "    ddp_rank = int(os.environ.get('RANK')) # process ID\n",
    "    ddp_local_rank = int(os.environ.get('LOCAL_RANK')) # id of the gpu on node, not used for now\n",
    "    ddp_world_size = int(os.environ.get('WORLD_SIZE')) # num of processes, i.e. number of gpus used\n",
    "    device = f'cuda:{ddp_local_rank}' # associates process with id:ddp_local_rank to the gpu:id\n",
    "    torch.cuda.set_device(device=device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc\n",
    "else:\n",
    "    # non ddp run\n",
    "    ddp_world_size = 1\n",
    "    ddp_rank, ddp_local_rank,  = 0, 0\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    master_process = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 128 #1024 # max seq len: max n of tokens processable by attention block\n",
    "    vocab_size: int = 50257 # n tokens 50k merges from BytePairEnc + 256 utf8 encodings + end_of_text token #! bad number\n",
    "    n_layer: int = 4 #12 # n of transformer blocks\n",
    "    n_head: int = 4 #12 # n of attention heads in each transformer block\n",
    "    n_embd: int = 128 #768 # embedding dimension of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4* config.n_embd) #! UPSAMPLING\n",
    "        self.gelu = nn.GELU(approximate='tanh') # approximate: i guess to be removed\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.weight_scaling = True # apply scaling of 1/sqrt(Num of residual layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    '''\n",
    "    multihead att\n",
    "    tri_low example:\n",
    "        (tensor([[[[1., 0., 0.],\n",
    "                    [1., 1., 0.],\n",
    "                    [1., 1., 1.]]]]),\n",
    "        torch.Size([1, 1, 3, 3]))\n",
    "    '''\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert config.n_embd % config.n_head == 0 #* idea: precise split n_embd in n_head chunks\n",
    "\n",
    "        # default bias: bool = True\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd) # 3 cuz kqv\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.weight_scaling = 1  # apply scaling of 1/sqrt(Num of residual layers)\n",
    "\n",
    "        # masking, named \"bias\" in OpenAI code\n",
    "\n",
    "        # low_tril = torch.tril(  # low_tril of 1s with diag 1, up_tril 0s\n",
    "        #     torch.ones(\n",
    "        #         config.block_size, config.block_size\n",
    "        #     ).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "        # self.register_buffer(\"bias\", low_tril)\n",
    "\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch_size, sequence_len, emb_dim\n",
    "\n",
    "        # compute KQV via single MLP, MultiHead as additional batch dimension\n",
    "\n",
    "        KQV = self.c_attn(x)\n",
    "        K, Q, V = KQV.split(self.n_embd, dim=-1) # out shape of each: B, T, n_head*n_embd\n",
    "\n",
    "        # \"promote\" n_head to batch dimension s.t. apply operations on heads as batches\n",
    "        # to do so we need to have tensor of shape (B, n_head, T, n_embd)\n",
    "        #* idea: split n_embd in n_head chunks\n",
    "        # hs = C // self.n_head eg 768 // 6 = 128\n",
    "\n",
    "        K = K.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        Q = Q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        V = V.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # get attention scores\n",
    "\n",
    "        # att = Q @ K.transpose(-2, -1) # now with the shapes got above this mutliplication occurs on all (B, nh)\n",
    "        # att /= math.sqrt(K.size(-1)) # normalize wrt n_embd size\n",
    "        # att = att.masked_fill(self.bias[ ... , :T, :T] == 0, float('-inf')) # mask \"future\" tokens, [ ... , :T, :T]: square matrix of token to token unnormalized attention\n",
    "        # att = F.softmax(att, dim=-1) # normalized: sums to 1\n",
    "        # y = att @ V # (B, nh,  T, T) @ (B, nh, T, hs), outs (B, nh, T, hs), \"demote\" nh back into n_embd, transpose back needed\n",
    "\n",
    "        y = F.scaled_dot_product_attention(Q,K,V, is_causal=True) # flash attention\n",
    "\n",
    "        # apply attention scores\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # .cat as in older implementation of multihead: return to C=self.n_head*hs\n",
    "\n",
    "        # out projection\n",
    "\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp  = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    decoder only architecture for autoregressive prediction of next token\n",
    "    clean residual flow from top-to-bottom is applied\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte  = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe  = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                h    = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), #! [Block(config)]*n fills list with same instance!\n",
    "                ln_f = nn.LayerNorm(config.n_embd)\n",
    "            ),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size) # default bias: bool = True\n",
    "\n",
    "        # weight sharing/tying scheme\n",
    "\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights) # recurisvely call this func on every submodule of self, look at docs of nn.Module.apply\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "\n",
    "        # default init of nn.LayerNorm is good\n",
    "\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=.02) # xavier: 1/sqrt(input dim of the layer)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            std = .02\n",
    "            if hasattr(module, 'weight_scaling'):\n",
    "                std *= (2 * self.config.n_layer ** -.5)\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, lr, device):\n",
    "        '''\n",
    "        idea behind wd: by forcing weights to be low u force the model to use all the feature vect dimensionality\n",
    "        do not wd biases and other 1d params such as LayerNorm params and such\n",
    "        u do want to wd matmul weights and embeddings\n",
    "        '''\n",
    "\n",
    "        # get all params that require grad\n",
    "\n",
    "        param_dict = {name:param for name, param in self.named_parameters() if param.requires_grad}\n",
    "\n",
    "        # split them according to their shape (which implies wheter to apply wd on them or not)\n",
    "\n",
    "        params_to_be_decayed = [p for p in param_dict.values() if p.dim()>=2]\n",
    "        params_NOT_to_be_decayed = [p for p in param_dict.values() if p.dim()<2]\n",
    "        optim_groups = [\n",
    "            {'params': params_to_be_decayed, 'weight_decay': weight_decay},\n",
    "            {'params': params_NOT_to_be_decayed, 'weight_decay': 0.0},\n",
    "        ]\n",
    "\n",
    "        # instanciate fused adamW\n",
    "        if device != 'cpu':\n",
    "            optimizer = torch.optim.AdamW(optim_groups, lr=lr, betas=[.9, .95], fused=True) # , eps=1e-8 just as the default\n",
    "        else:\n",
    "            optimizer = torch.optim.AdamW(optim_groups, lr=lr, betas=[.9, .95], fused=False) # , eps=1e-8 just as the default\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "\n",
    "        '''\n",
    "        idx is the sentence casted to tokens\n",
    "        '''\n",
    "\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cant't process seqs with num of tokens larger than {T}\"\n",
    "\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos) # T, n_embd\n",
    "        idx_emb = self.transformer.wte(idx) # B, T, n_embd\n",
    "        x = pos_emb + idx_emb # broadcasting pos_emb over the batch dim\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x) # B, T, n_embd\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            flatten_logits = logits.view(-1, logits.size(-1))\n",
    "            flatten_targets = targets.view(-1)\n",
    "            # idea: logits shape: (B, T, vocab_size), targets: (B, T)\n",
    "            # get logits as: (B*T, vocab_size) and targets as: (B*T,) of correct idxs\n",
    "            loss = F.cross_entropy(flatten_logits, flatten_targets )\n",
    "\n",
    "        return logits, loss # B, T, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x, max_len = 30):\n",
    "    '''\n",
    "    minimal example from topk to gather:\n",
    "    x: shape(2, 5), already softmaxed\n",
    "    x:tensor([[0.1738, 0.1881, 0.4180, 0.0235, 0.1965],\n",
    "              [0.1004, 0.0591, 0.0454, 0.4973, 0.2977]])\n",
    "\n",
    "    topk(k = 3):\n",
    "    topk_probs: tensor([[0.4180, 0.1965, 0.1881],\n",
    "                        [0.4973, 0.2977, 0.1004]])\n",
    "    topk_idxs:  tensor([[2, 4, 1],\n",
    "                        [3, 4, 0]])\n",
    "\n",
    "    multinomial sampling out of topk_probs\n",
    "    ix: tensor([[0],\n",
    "                [0]])\n",
    "    ix: idxs used to retrieve sampled value out of topk_idxs\n",
    "\n",
    "    collect results with torch.gather:\n",
    "    xcol: tensor([[2],\n",
    "                  [3]])\n",
    "    xcol: tokens sampled out of vocab\n",
    "    '''\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while x.size(1) < max_len:\n",
    "            logits = model(x)\n",
    "            logits = logits[:, -1, :] # select all last tokens\n",
    "            probs = F.softmax(logits, dim=-1) # get prob dist over last token type\n",
    "\n",
    "            # keep only top 50 out of softmax(last_token) of size: vocab_size s.t. drop watherver is too low\n",
    "            # returns prob values and their associated idxs in the original tensor\n",
    "            topk_probs, topk_idxs = probs.topk(50, dim=-1)\n",
    "\n",
    "            ix = torch.multinomial(topk_probs, num_samples=1) # actual sampling, does #num_samples per obs, returns idx of element sampled out of topk_probs\n",
    "            # thus the ix above returned have to be used to first get the associated value in topk_idxs\n",
    "            xcol = torch.gather(topk_idxs, dim=-1, index=ix) # get topk_idxs that has been sampled by torch.multinomial, Gathers values along an axis specified by dim\n",
    "            x = torch.cat((x, xcol), dim=1) # append col of just-sampled tokens to input and refeed to model\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def decode(x):\n",
    "    for i in range(num_return_seqs):\n",
    "        tokens = x[i, :max_len].tolist()\n",
    "        decoded = enc.decode(tokens)\n",
    "        print('>', decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataLoaderLite:\n",
    "\n",
    "#     def __init__(self, B, T):\n",
    "\n",
    "#         # good batch sizes: 8 16 24 32 48 64 128 256\n",
    "\n",
    "#         self.B = B\n",
    "#         self.T = T\n",
    "\n",
    "#         tiny_shakps = '/home/marconobile/Videos/courses/karpathy/data/input.txt'\n",
    "#         with open(tiny_shakps, 'r') as f:\n",
    "#             text = f.read()\n",
    "\n",
    "#         # tokenize it\n",
    "\n",
    "#         encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "#         encoded_text = encoder.encode(text)\n",
    "#         self.tokens = torch.tensor(encoded_text, dtype=torch.long)\n",
    "\n",
    "#         print(f'Loaded {len(self.tokens)} tokens')\n",
    "#         print(f'1 epoch has: {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "#         # text cursor\n",
    "#         self.current_position = 0\n",
    "\n",
    "#     def next_batch(self):\n",
    "#         B, T = self.B, self.T\n",
    "\n",
    "#         # read text at chunks of B*T tokens\n",
    "#         # idea: load enough tokens to fill a batch of size B with sequences of lenght T\n",
    "#         # +1 to get label of last token in buffer\n",
    "\n",
    "#         buffer = self.tokens[self.current_position: self.current_position + B*T + 1]\n",
    "#         x = buffer[:-1].view(B, T)\n",
    "#         y = buffer[1:].view(B, T)\n",
    "\n",
    "#         # avance text_ptr by B*T\n",
    "#         # if at end of file: restart\n",
    "\n",
    "#         self.current_position += B*T # shift by how much text has been read\n",
    "#         if self.current_position + (B*T + 1) > len(self.tokens):\n",
    "#             self.current_position = 0\n",
    "\n",
    "#         return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "\n",
    "    def __init__(self, B, T, process_rank, num_processes):\n",
    "\n",
    "        # good batch sizes: 8 16 24 32 48 64 128 256\n",
    "\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "\n",
    "        tiny_shakps = '/home/marconobile/Videos/courses/karpathy/data/input.txt'\n",
    "        with open(tiny_shakps, 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize it\n",
    "\n",
    "        encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "        encoded_text = encoder.encode(text)\n",
    "        self.tokens = torch.tensor(encoded_text, dtype=torch.long)\n",
    "\n",
    "        print(f'Loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch has: {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # text cursor\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "        # so eg;\n",
    "        # if gpu 0-> self.process_rank= 0 self.current_position = 0\n",
    "        # if gpu 1-> self.process_rank= 0 self.current_position = self.B * self.T\n",
    "        # if gpu 2-> self.process_rank= 0 self.current_position = self.B * self.T * 2\n",
    "        # thus we have proc0 that reads (0, self.B * self.T)\n",
    "        # thus we have proc1 that reads (self.B * self.T, self.B * self.T*2)\n",
    "        # thus we have proc2 that reads (self.B * self.T, self.B * self.T*3)\n",
    "        # BS:2, T=4\n",
    "        # [\n",
    "        #   [0,0,0,0],[1,1,1,1],[2,2,2,2], [0,0,0,0],[1,1,1,1],[2,2,2,2],\n",
    "        #   [0,0,0,0],[1,1,1,1],[2,2,2,2], [0,0,0,0],[1,1,1,1],[2,2,2,2],\n",
    "        # ]\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # read text at chunks of B*T tokens\n",
    "        # idea: load enough tokens to fill a batch of size B with sequences of lenght T\n",
    "        # +1 to get label of last token in buffer\n",
    "\n",
    "        buffer = self.tokens[self.current_position: self.current_position + B*T + 1]\n",
    "        x = buffer[:-1].view(B, T)\n",
    "        y = buffer[1:].view(B, T)\n",
    "\n",
    "        # avance text_ptr by B*T\n",
    "        # if at end of file: restart\n",
    "\n",
    "        self.current_position += B * T * self.num_processes # shift by how much text has been read\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_position = self.B * self.T * self.process_rank\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = list(range(100))\n",
    "# y = [get_lr(step) for step in x]\n",
    "# print(y[-1])\n",
    "# plt.plot(x, y)\n",
    "\n",
    "# print(loss, -math.log(1/config.vocab_size))\n",
    "# tensor(11.1026, grad_fn=<NllLossBackward0>) 10.82490511970208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 338025 tokens\n",
      "1 epoch has: 1320 batches\n"
     ]
    }
   ],
   "source": [
    "import torch.distributed\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "num_return_seqs = 2\n",
    "max_len = 30\n",
    "config = GPTConfig(vocab_size=50304) # cuz more powers of 2 then 50257\n",
    "# this does not change the funcitoning of the model cuz at inpt thse fake tokens will never be accessed\n",
    "# at out they will never get predicted and their associated weights are just driven to -inf (the bias in particular)\n",
    "# with more mflops we are faster cuz of cuda kernel implementations with odd numbers\n",
    "model = GPT(config)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "if is_ddp:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[ddp_local_rank]) # synchronizes and avg the grad on each model over the devices\n",
    "raw_model = model.module if is_ddp else model\n",
    "\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * .1 # 10% of max_lr\n",
    "\n",
    "warmup_steps = 10\n",
    "max_steps = 50\n",
    "\n",
    "def get_lr(step):\n",
    "\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step+1) / warmup_steps # linear increase in lr for n_warmup_steps\n",
    "\n",
    "    # 2) if step > lr_decay_iters, return min lr\n",
    "\n",
    "    if step > max_steps:\n",
    "        return min_lr\n",
    "\n",
    "    # 3) in between use cosine decay down to min_lr\n",
    "\n",
    "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = .5 * (1.0 + math.cos(math.pi * decay_ratio)) # starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "norms = []\n",
    "optimizer = raw_model.configure_optimizers(.1, max_lr, device)\n",
    "\n",
    "# gradient accumulation param\n",
    "B, T = 2, 128 # microbs\n",
    "total_bs = 524288\n",
    "assert total_bs % (B * T * ddp_world_size) == 0, \"total_bs must be divisible by B*T*ddp_world_size\"\n",
    "grad_accum_steps = total_bs // (B * T * ddp_world_size)  # number of times that fwd/bckwd steps are done and grad +=\n",
    "                                                         # now we have ddp_world_size processes so we process (B * T) tokens on each gpu\n",
    "                                                         # nb: ddp_world_size is the count of GPU\n",
    "\n",
    "train_loader = DataLoaderLite(\n",
    "    B=B,\n",
    "    T=T,\n",
    "    process_rank = ddp_rank,\n",
    "    num_processes = ddp_world_size,\n",
    ")\n",
    "\n",
    "\n",
    "for step in range(max_steps):\n",
    "\n",
    "    loss_accum = 0.0\n",
    "\n",
    "    # mandatory\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "\n",
    "        # get data from data loader and ship it to cuda\n",
    "\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # get gradients\n",
    "        if is_ddp:\n",
    "            # the idea here is that without this line ddp synchronizes at each step the processes, but actually we want to sync them only at last step b4 application of grad\n",
    "            # this field is also used by the forward pass\n",
    "            model.require_backward_grad_sync = (micro_step == grad_accum_steps -1) # this is just bcuz we are using gradient accumulation\n",
    "\n",
    "        # forward step with mixed precision\n",
    "\n",
    "        with torch.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', dtype=torch.bfloat16):\n",
    "            logits, loss = model(x,y)\n",
    "\n",
    "        # normalize wrt grad_accum_steps \"un-average\"\n",
    "\n",
    "        loss /= grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "    # when we get here all devices will have gradients that have been computed on the other devices, but now we need to avg the loss computed on the diff devices\n",
    "    if is_ddp:\n",
    "        torch.distributed.all_reduce(loss_accum, op=torch.distributed.ReduceOp.AVG) # all devices now will have the overall loss_accum avgd\n",
    "\n",
    "    # clip grad norm\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm(model.parameters(), 1.0) # avoid shocks to the model during optimization; PRINTABLE expected trend from high to low and stabilize\n",
    "    norms.append(norm.item())\n",
    "\n",
    "    # define lr for this step\n",
    "    lr = get_lr(step)\n",
    "    for pg in optimizer.param_groups:\n",
    "        pg['lr'] = lr\n",
    "\n",
    "    # optimize\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    n_tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
    "\n",
    "    if master_process:\n",
    "        print(f'Step {step} loss: {loss_accum.item()}, grad_norm: {norm.item()}')\n",
    "\n",
    "if is_ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathyAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

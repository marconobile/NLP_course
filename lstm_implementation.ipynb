{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM in numpy\n",
    "\n",
    "[\n",
    "    h_pre_lstm_cell = W_hh @ h_t-1 + W_hx @ x_t + b_h\n",
    "\n",
    "    input gate  = i_t = sigmoid(h_pre_lstm_cell)  : how much/what to retain/input to the cell state of x_t, h_t-1\n",
    "    forget gate = f_t = sigmoid(h_pre_lstm_cell)  : how much/what to forget of the prev cell state as a funct of x_t, h_t-1\n",
    "    cell gate   = g_t = tanh(h_pre_lstm_cell) \n",
    "    output gate = o_t = sigmoid(h_pre_lstm_cell) \n",
    "]\n",
    "\n",
    "IMPO: cell state update rule\n",
    "- cell state c_t = element_wise_prod(c_t-1 * f_t) + element_wise_prod(i_t * g_t)\n",
    "\n",
    "- element_wise_prod(c_t-1 * f_t):\n",
    "cell state updated by scalindg down prev cell state acccording to f_t; f_t defines how much to forget; f_t is a funct of x_t, h_t-1 \\\n",
    "    if f_t = torch.ones -> all prev cell state is carried on \\\n",
    "    if f_t = torch.zeros -> all prev cell state dies \\\n",
    "with proper weight to learn forget gate rule we learn what to keep/discard from memory \n",
    "- hidden state c_t = element_wise_prod(tanh(c_t) * o_t)\n",
    "\n",
    "\n",
    "pytorch implements the h_pre_lstm_cell computation using a single matrix mul and then splitting the output matrix\n",
    "\n",
    "core idea:\n",
    "- sigmoid outputs values in [0, 1]\n",
    "- tanh outputs values in [-1, +1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "XFEATURES = 20\n",
    "HIDDEN_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_hh  torch.Size([400, 100])\n",
      "weight_ih  torch.Size([400, 20])\n",
      "bias_hh  torch.Size([400])\n",
      "bias_ih  torch.Size([400])\n"
     ]
    }
   ],
   "source": [
    "model = nn.LSTMCell(XFEATURES, HIDDEN_SIZE)\n",
    "print(\"weight_hh \", model.weight_hh.shape)\n",
    "print(\"weight_ih \", model.weight_ih.shape)\n",
    "print(\"bias_hh \", model.bias_hh.shape)\n",
    "print(\"bias_ih \", model.bias_ih.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):  return 1 / (1 + np.exp(-x))\n",
    "def check_equal(a, b): return np.linalg.norm(a.detach().numpy() - b) < 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(x, h, c, W_hh, W_ih, b):\n",
    "    # single lstm cell == single layer of lstm\n",
    "    # can process only a single timestep/element x_i of a sequence of lenght T\n",
    "    i, f, g, o = np.split(W_hh@h + W_ih@x + b, 4)\n",
    "    i, f, g, o = sigmoid(i), sigmoid(f), np.tanh(g), sigmoid(o)\n",
    "    c_out = f * c + i * g\n",
    "    h_out = o * np.tanh(c_out)\n",
    "    return h_out, c_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(BATCH_SIZE, XFEATURES).astype(np.float32)\n",
    "h0 = np.random.randn(BATCH_SIZE, HIDDEN_SIZE).astype(np.float32) # initial hidden state\n",
    "c0 = np.random.randn(BATCH_SIZE, HIDDEN_SIZE).astype(np.float32) # initial cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 100]), torch.Size([3, 100]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_true, c_true = model(torch.tensor(x), (torch.tensor(h0), torch.tensor(c0)))\n",
    "h_true.shape, c_true.shape # and indeed the out h is vector of shape (BATCH_SIZE, HIDDEN_SIZE): 1 timestep for each obs in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, c = lstm_cell(x[0], h0[0], c0[0],\n",
    "                 model.weight_hh.detach().numpy(),\n",
    "                 model.weight_ih.detach().numpy(),\n",
    "                 model.bias_hh.detach().numpy()+model.bias_ih.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,), (100,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_equal(h_true, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "XFEATURES = 20\n",
    "HIDDEN_SIZE = 100\n",
    "SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so with a single lstm cell we are able to process a single timestep for a whole batch of sequences, now with by replicating the nn.LSTM module\n",
    "# we will be able to process a whole sequence with a single forward call\n",
    "# the idea is that we are going to call step after step LSTMCell\n",
    "model = nn.LSTM(XFEATURES, HIDDEN_SIZE, num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(SEQ_LEN, XFEATURES).astype(np.float32)\n",
    "h0 = np.random.randn(BATCH_SIZE, HIDDEN_SIZE).astype(np.float32) # initial hidden state\n",
    "c0 = np.random.randn(BATCH_SIZE, HIDDEN_SIZE).astype(np.float32) # initial cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(X, h, c, W_hh, W_ih, b):\n",
    "    # returns an array containing all the hidden states at computed at each time step\n",
    "    # returns only the last cell state (i.e. the cell state obtained at the last timestep of the sequence)\n",
    "    H = np.zeros((SEQ_LEN, HIDDEN_SIZE))\n",
    "    for t in range(SEQ_LEN):\n",
    "        h, c = lstm_cell(X[t], h, c, W_hh, W_ih, b)\n",
    "        H[t] = h # at each time step we store away the hidde state obtained\n",
    "    return H, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, cn = lstm(X, h0[0], c0[0],\n",
    "                model.weight_hh_l0.detach().numpy(),\n",
    "                model.weight_ih_l0.detach().numpy(),\n",
    "                model.bias_hh_l0.detach().numpy()+model.bias_ih_l0.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to manually insert batch size cuz it is required by nn.LSTM\n",
    "H_true, (h_true, c_true) = model(torch.tensor(X)[:,None,:],\n",
    "                                (torch.tensor(h0)[:,None,:],\n",
    "                                 torch.tensor(c0)[:,None,:]))\n",
    "# nn.LSTM returns both the full matrix of hiddent stats and tuple of last hidden and last cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_equal(H_true[:,0,:], H), check_equal(c_true, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 100])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_true.shape # returns hidden states for each timestep for each sequence in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for optimization reasons (contiguity in memory) we store input data for LSTM in form:\n",
    "# X[Timesteps, Batch_Size, x_i_feat] # Len, N_obs, H_in from pytorch docs notation\n",
    "# so we can get at the timestep t for the whole batch as: X[t,:,:]\n",
    "# thus we need to change our implementations:\n",
    "# X[SEQ_LEN, BATCH_SIZE, XFEATURES]\n",
    "# H[SEQ_LEN, BATCH_SIZE, HIDDEN_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(x, h, c, W_hh, W_ih, b):\n",
    "    i, f, g, o = np.split(h@W_hh + x@W_ih + b, 4, axis=1)\n",
    "    i, f, g, o = sigmoid(i), sigmoid(f), np.tanh(g), sigmoid(o)\n",
    "    c_out = f*c + i*g\n",
    "    h_out = o * np.tanh(c_out)\n",
    "    return h_out, c_out\n",
    "\n",
    "def lstm(X, h, c, W_hh, W_ih, b):\n",
    "    # returns an array containing all the hidden states at computed at each time step\n",
    "    # returns only the last cell state (i.e. the cell state obtained at the last timestep of the sequence)\n",
    "    H = np.zeros((SEQ_LEN, BATCH_SIZE, HIDDEN_SIZE))\n",
    "    for t in range(SEQ_LEN):\n",
    "        h, c = lstm_cell(X[t], h, c, W_hh, W_ih, b)\n",
    "        H[t] = h # at each time step we store away the hidde state obtained\n",
    "    return H, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "XFEATURES = 20\n",
    "HIDDEN_SIZE = 100\n",
    "SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(SEQ_LEN, BATCH_SIZE, XFEATURES).astype(np.float32)\n",
    "h0 = np.random.randn(1, BATCH_SIZE, HIDDEN_SIZE).astype(np.float32) # initial hidden state\n",
    "c0 = np.random.randn(1, BATCH_SIZE, HIDDEN_SIZE).astype(np.float32) # initial cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_true, (h_true, c_true) = model(torch.tensor(X),\n",
    "                                (torch.tensor(h0),\n",
    "                                 torch.tensor(c0)))\n",
    "# nn.LSTM returns both the full matrix of hidden states and a tuple of: (hidden state at last timestep and cell state at last timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 100])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_true.shape # returns hidden states for each timestep for each sequence in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, cn = lstm(X, h0[0], c0[0],\n",
    "                model.weight_hh_l0.detach().numpy().T,\n",
    "                model.weight_ih_l0.detach().numpy().T,\n",
    "                model.bias_hh_l0.detach().numpy()+model.bias_ih_l0.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_equal(H_true, H), check_equal(c_true, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathyAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

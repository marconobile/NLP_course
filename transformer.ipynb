{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "Let's begin by implementing \n",
    "$$ y = softmax(\\frac{QK^{T}}{\\sqrt(d)}) V$$\n",
    "\n",
    "the main idea is to create attention just as a module/layer on its own. It takes as input x and applies attention on it, with the following conventional form:\n",
    "$$ y = (softmax(\\frac{xW_{K} * W_{Q}x}{\\sqrt(d)}) W_{v}x) W_{out}$$\n",
    "\n",
    "where $$W_{out}$$ is just a linear projection layer to project the new representations obtained via attention mechanism in another dimensional space. This could be integrated inside $$ W_{V} $$ but is better to do not and to divide the 2 different tasks (in particular in the multihead attention case).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    Z = np.exp(Z- Z.max(axis=-1, keepdims=True))\n",
    "    return Z / Z.sum(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(X, mask, W_KQV, W_out):\n",
    "    K,Q,V = np.split(X @ W_KQV, 3, axis=1) # instead of doing 3 mm, is better to do a single large and then split\n",
    "    attn = softmax(K @ Q.T / np.sqrt(X.shape[1]) + mask) # mask to avoid spilling info from current/past timesteps to future\n",
    "    return attn @ V @ W_out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.MultiheadAttention?\n",
    "# Init signature:\n",
    "# nn.MultiheadAttention(\n",
    "#     embed_dim,\n",
    "#     num_heads,\n",
    "#     dropout=0.0,\n",
    "#     bias=True,\n",
    "#     add_bias_kv=False,\n",
    "#     add_zero_attn=False,\n",
    "#     kdim=None,\n",
    "#     vdim=None,\n",
    "#     batch_first=False,\n",
    "#     device=None,\n",
    "#     dtype=None,\n",
    "# ) -> None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "NELEMS, FEATURES = 100, 64\n",
    "pyt_attn = nn.MultiheadAttention(embed_dim=FEATURES, num_heads=1, bias=False, batch_first=True)\n",
    "M = torch.triu(-float(\"inf\")* torch.ones(NELEMS, NELEMS), diagonal=1)\n",
    "\n",
    "X = torch.randn(1, NELEMS, FEATURES)\n",
    "Y_true, attn_true = pyt_attn(X, X, X, attn_mask=M) # multihead_attn(query, key, value)\n",
    "# attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, attn = self_attention(X[0].numpy(), M.numpy(),\n",
    "                                   pyt_attn.in_proj_weight.detach().numpy().T,\n",
    "                                   pyt_attn.out_proj.weight.detach().numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.74691534, 0.2530846 , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.23462467, 0.17931736, 0.58605796, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.01249573, 0.02230426, 0.00682052, ..., 0.00766308, 0.        ,\n",
       "        0.        ],\n",
       "       [0.01006045, 0.00422531, 0.00532255, ..., 0.02608492, 0.01317528,\n",
       "        0.        ],\n",
       "       [0.00710144, 0.01114072, 0.00821384, ..., 0.00815385, 0.03464449,\n",
       "        0.00397532]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6083706e-07"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(attn - attn_true.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1628191e-06"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(Y - Y_true.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(X, mask, W_KQV, W_out):\n",
    "    K,Q,V = np.split(X @ W_KQV, 3, axis=-1) # instead of doing 3 mm, is better to do a single large and then split\n",
    "    attn = softmax(K @ Q.swapaxes(-1, -2) / np.sqrt(X.shape[-1]) + mask) # mask to avoid spilling info from current/past timesteps to future\n",
    "    return attn @ V @ W_out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH, NELEMS, FEATURES = 50, 100, 64\n",
    "pyt_attn = nn.MultiheadAttention(embed_dim=FEATURES, num_heads=1, bias=False, batch_first=True)\n",
    "M = torch.triu(-float(\"inf\")* torch.ones(NELEMS, NELEMS), diagonal=1)\n",
    "\n",
    "X = torch.randn(BATCH, NELEMS, FEATURES)\n",
    "Y_true, attn_true = pyt_attn(X, X, X, attn_mask=M) # multihead_attn(query, key, value)\n",
    "# attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, attn = self_attention(X.numpy(), M.numpy(),\n",
    "                                   pyt_attn.in_proj_weight.detach().numpy().T,\n",
    "                                   pyt_attn.out_proj.weight.detach().numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5335097e-06"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(attn - attn_true.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0895182e-05"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(Y - Y_true.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in single head attention each el of the Q@K.T matrix is the dot prod between q_i and k_j"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathyAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

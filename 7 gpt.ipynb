{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links:\n",
    "- https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=hoelkOrFY8bN\n",
    "- https://github.com/Matjaz12/GPT-Explained/blob/main/GPT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data and Preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-08 18:10:12--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.4’\n",
      "\n",
      "input.txt.4         100%[===================>]   1.06M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2024-01-08 18:10:13 (13.2 MB/s) - ‘input.txt.4’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download data:\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f: text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text) # len of data i.e. num of chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000] # first 1k chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# let's get vocab\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_len = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64} {0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "[46, 43, 50, 50, 53, 61, 1, 61, 53, 56, 50, 42, 2, 4, 2]\n",
      "hellow world!&!\n"
     ]
    }
   ],
   "source": [
    "# let's tokenize text at the char level\n",
    "# build mappings\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for ch,i in stoi.items()}\n",
    "print(stoi, itos)\n",
    "\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode('hellow world!&!'))\n",
    "print(decode(encode('hellow world!&!')))\n",
    "# there are many tokenization schemes eg google uses SentencePiece (sub-word tokenizer), openAI uses tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so now we can tokenize the input corpus\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's split data in train/test/val\n",
    "n = int(.9*len(data))\n",
    "train_data = data[:n] # 90%\n",
    "val_data = data[n:] # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_len = 8\n",
    "train_data[:ctx_len+1] # a first example of input data\n",
    "# here we have that 47 comes after 18, 56 comes after 18 and 47, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:ctx_len]\n",
    "y = train_data[1:ctx_len+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18, 47, 56, 57, 58,  1, 15, 47]),\n",
       " tensor([47, 56, 57, 58,  1, 15, 47, 58]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y\n",
    "# x0 x1 x2 x3 x4 x5 x6 x7\n",
    "#   /  /  /  /  /  /  /\n",
    "# x1 x2 x3 x4 x5 x6 x7 x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Context: tensor([18]), target: 47\n",
      "Sample 1, Context: tensor([18, 47]), target: 56\n",
      "Sample 2, Context: tensor([18, 47, 56]), target: 57\n",
      "Sample 3, Context: tensor([18, 47, 56, 57]), target: 58\n",
      "Sample 4, Context: tensor([18, 47, 56, 57, 58]), target: 1\n",
      "Sample 5, Context: tensor([18, 47, 56, 57, 58,  1]), target: 15\n",
      "Sample 6, Context: tensor([18, 47, 56, 57, 58,  1, 15]), target: 47\n",
      "Sample 7, Context: tensor([18, 47, 56, 57, 58,  1, 15, 47]), target: 58\n"
     ]
    }
   ],
   "source": [
    "for t in range(ctx_len):\n",
    "    ctx = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Sample {t}, Context: {ctx}, target: {target}\") # so given a single chunk of the train data within a contex block we have 8 samples\n",
    "    # it is important to train with all data with context between 1 and ctx_size cuz transformer must be able to adapt to any input size\n",
    "    # thus we wrap up all these samples in a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[42, 39, 56, 43,  1, 52, 53, 58],\n",
       "         [63, 53, 59, 56,  1, 60, 53, 47],\n",
       "         [ 6,  1, 46, 43, 56,  1, 44, 39],\n",
       "         [54, 58, 59, 56, 52, 43, 42,  1]]),\n",
       " tensor([[39, 56, 43,  1, 52, 53, 58,  1],\n",
       "         [53, 59, 56,  1, 60, 53, 47, 41],\n",
       "         [ 1, 46, 43, 56,  1, 44, 39, 58],\n",
       "         [58, 59, 56, 52, 43, 42,  1, 61]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_data\n",
    "ix = torch.randint(len(data) - ctx_len, (4,)) # up to last char - ctx block s.t. have a complete block even at the end of dataset\n",
    "x = torch.stack([data[i:i+ctx_len]  for i in ix])\n",
    "y = torch.stack([data[i+1:i+ctx_len+1]  for i in ix])\n",
    "x, y\n",
    "\n",
    "# so we here we have 32 samples (bs * ctx_len) cuz for each (x[i, 0:j] for j from 0 to ctx_len) we have a yij (look above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "bs = 4\n",
    "ctx_len = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - ctx_len, (bs,)) # up to last char - ctx block s.t. have a complete block even at the end of dataset\n",
    "    x = torch.stack([data[i:i+ctx_len]  for i in ix])\n",
    "    y = torch.stack([data[i+1:i+ctx_len+1]  for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            losses[i] = loss\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: BigramLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    '''\n",
    "    The model learns each entry of a (vocab_len, vocab_len) table\n",
    "    where each entry is the probability dist of the following char given an input char at row\n",
    "    '''\n",
    "    def __init__(self, vocab_len):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (bigram model lecture 2)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_len, vocab_len)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are int tensors of shape (bs, ctx_len)\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets == None: return logits, None\n",
    "\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    # this function will change over the course of the lecture\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (bs, ctx_len) array of int-idxs that define the context\n",
    "        # these int-idxs are chars from the vocab\n",
    "        # in the bigram model only 1 char is looked at as ctx\n",
    "        for _ in range(max_new_tokens):\n",
    "            # predict i.e. get unnormalized probs\n",
    "            logits, _ = self(idx)\n",
    "            # get last time step\n",
    "            logits = logits[:, -1, :] # (bs, out_classes), -1 cuz rn we are using only the last char in the bigram model\n",
    "            # normalize probs\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (bs,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (bs, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
       "         [44, 53, 56,  1, 58, 46, 39, 58],\n",
       "         [52, 58,  1, 58, 46, 39, 58,  1],\n",
       "         [25, 17, 27, 10,  0, 21,  1, 54]]),\n",
       " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
       "         [53, 56,  1, 58, 46, 39, 58,  1],\n",
       "         [58,  1, 58, 46, 39, 58,  1, 46],\n",
       "         [17, 27, 10,  0, 21,  1, 54, 39]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, yb.shape)\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.677961826324463"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_len=vocab_len)\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = m(xb, yb)\n",
    "logits.shape # for each chunk of text selected (bs = 4) of size ctx_len (8), we deconstruct the text in a sequential manner s.t. create\n",
    "# 8 samples so for each one of the 8*4=32 samples we get a vocab_size tensor that represent the prob dist of over the next char\n",
    "# all of these given that we are using directly embeddings is just as indexing into the token_embedding_table\n",
    "\n",
    "loss.item() # we know that the initial loss  must be -math.log(1/vocab_len) = 4.174387269895637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nkrENNTjLDuQcLzy'RIo;'KdhpV\\nvLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq!Lzn\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long) # torch.long = int; 0 is \\n so good char to begin generation\n",
    "decode(m.generate(idx, max_new_tokens=100)[0].tolist())\n",
    "# atm trash cuz not trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marconobile/miniconda3/envs/karpathyAI/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400366987/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated train loss: 4.641025066375732, estimated val loss: 4.660730361938477\n",
      "Estimated train loss: 2.822432041168213, estimated val loss: 2.850165367126465\n",
      "Estimated train loss: 2.5394654273986816, estimated val loss: 2.5837109088897705\n",
      "Estimated train loss: 2.504521369934082, estimated val loss: 2.495387315750122\n",
      "Estimated train loss: 2.48500394821167, estimated val loss: 2.5055902004241943\n",
      "Estimated train loss: 2.4403717517852783, estimated val loss: 2.4746057987213135\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_num_steps = 25001\n",
    "eval_interval = 5000\n",
    "for step in range(max_num_steps):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if step % eval_interval == 0:\n",
    "        out = estimate_loss(m)\n",
    "        print(f\"Estimated train loss: {out['train']}, estimated val loss: {out['val']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nBUnsist w; miome!\\nGQUps anomahall wherince ithity, st: ginodishelodeas s hengofrof S:\\n3Be topof qulcadusullowompr Lein I schivefio te aine sther tho Apl\\nAD otoese s MPe '?\\nWig paiceneelin g se?\\nOMELid y, p't ineay epevend me,\\nOur oulel yo n at, fef und 'Whaithe thoounthasindstre ge spld my\\npre t ge\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "decode(m.generate(idx, max_new_tokens=300)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that we want chars/tokens to talk to each other to generate a meaningful context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now see an important mathematical trick at the hearth of __self-attention__ implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2 # batch, tokens, token dimensionality\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing to notice is that context can flow only flow from past up untill current token (we are trying to predict next word, we cannot use it as context). Thus context for current token is only retrospective.\n",
    "A naive approach would be to take an average of current token and all feature vectors of previously processed tokens, s.t. get a form or retrospective context. Naive cuz we loose positionality, ordering and we take all info as equal. \n",
    "For now let's implement this naive average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want x[B, T] = mean_{i<=t>} x[B, i]\n",
    "xbow = torch.zeros(B, T, C) # C token dimensionality\n",
    "for b in range(B): # for each obs in the batch\n",
    "    for t in range(T): # for each token in obs\n",
    "        xprev = x[b, :t+1] # select the given obs, up untill current token, current included, (t, C)\n",
    "        xbow[b,t] = xprev.mean(0) # take avg of selected tokens, go next token\n",
    "# recall here that if we have\n",
    "# [\n",
    "#     x1: [x11, x12],\n",
    "#     x2: [x21, x22],\n",
    "#     x3: [x31, x32],\n",
    "#     x4: [x41, x42],\n",
    "#     x5: [x51, x52]\n",
    "# ]\n",
    "\n",
    "# then each i-th row of the resulting matrix is the col-wise average up until the i-th row of the data matrix\n",
    "# [\n",
    "#     avg(x1):             [x11, x12]/1,\n",
    "#     avg(x1,x2):          [x11+x21, x12+x22]/2,\n",
    "#     avg(x1,x2,x3):       [x11+x21+x31, x12+x22+x32]/3,\n",
    "#     avg(x1,x2,x3,x4):    [x11+x21+x31+x41, x12+x22+x32+x42]/4,\n",
    "#     avg(x1,x2,x3,x4,x5): [x11+x21+x31+x41+x51, x12+x22+x32+x42+x52]/5\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b =\n",
      "tensor([[2., 7.],\n",
      "        [6., 6.],\n",
      "        [1., 4.]])\n",
      "--\n",
      "c =\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 6.5000],\n",
      "        [3.0000, 5.6667]])\n"
     ]
    }
   ],
   "source": [
    "# the above is sound and good but inefficient, we want to find a way to do it with a matrix multiplication:\n",
    "# toy example illustrating how matrix multiplication can be used for a cumulative average over the stream of tokens\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True) # become \"weights\"\n",
    "b = torch.randint(0,10,(3,2)).float() # feature vectors\n",
    "c = a @ b\n",
    "print('a =')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b =')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c =')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# so let's vectorize the for-loop:\n",
    "weights = torch.tril(torch.ones(T,T))\n",
    "weights = weights / torch.sum(weights, 1, keepdim=True)\n",
    "print(weights)\n",
    "\n",
    "xbow2 = weights @ x\n",
    "# (T,T) @ (B, T, C) thus pytorch broadcasts weights to make it (B,T,T) i.e. creates B (T,T) matrices and stacks them up\n",
    "# so we get (B,T,T) @ (B,T,C) = (B,T,C) <- which was the shape of x\n",
    "# so we have obtained a new repr of each token simple-averaging itself with its past\n",
    "# we can see this simple mean as a weighted sum where weights in this case are 1/n_past_tokens\n",
    "# we could use some more smart aggregation rules instead of simple-mean weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's rewrite the same thing as above with the softmax\n",
    "# why? -> see next cell\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "weights = torch.zeros((T,T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # put -inf in all positions where the tril has 0s\n",
    "print(weights)\n",
    "\n",
    "# then we apply softmax over each row\n",
    "# e**0 = 1; e-inf = 0 so we replicate the exact weights from above\n",
    "weights = weights.softmax(-1) # 0 is over the cols, 1 over the rows\n",
    "print(weights)\n",
    "xbow3 = weights @ x\n",
    "\n",
    "torch.allclose(xbow, xbow2)\n",
    "torch.allclose(xbow, xbow3)\n",
    "torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why we want to use this form?\n",
    "\n",
    "# let's consider:\n",
    "weights = torch.zeros((T,T)) #\n",
    "# the weights start from a tensor of 0s, and the idea is that this vector (which has tril shape), represent the strenght of\n",
    "# the \"correlation\"/interaction-strenght/affinity\n",
    "# of each past token wrt current token. Or better it represent how much of each past token we want to consider/aggregate to be averaged up\n",
    "# in the computation of the context of the current token.\n",
    "\n",
    "# let's consider:\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # put -inf in all positions where the tril has 0s\n",
    "weights = weights.softmax(-1)\n",
    "# the idea here is that tokens from the future cannot interact/used/averaged to compute ctx of current token\n",
    "\n",
    "# the aggregation step\n",
    "xbow3 = weights @ x\n",
    "\n",
    "# the idea is that we are going to learn the cofficients of the weighted average, and these coefficients are going to be called\n",
    "# affinities or attention coeffs. These coeffs will be data dependant and will be defined by how much a each token is interested to other past tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention head/block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want gather information from the past in a data dependent way. How?\n",
    "# Each token/node emits 2 vectors:\n",
    "# - queries: what am I looking for\n",
    "# - keys: what do I contain\n",
    "\n",
    "# affinities: dot prod between keys and querys\n",
    "# so my query dotprod with all the keys of all the other tokens defines the weights matrix. The idea is that if the dot prod is high, then it means that the key matches the query\n",
    "# Example: if the query of tokenA has high dot prod with the key of a past tokenB, then the row of the weights/attention coeffs matrix where the tokenA is the last token considered\n",
    "# will define a high value for the idx-position of tokenB (nb the weights sum to 1)\n",
    "\n",
    "# single head perform self attention\n",
    "\n",
    "# set up fake data\n",
    "B, T, C = 4, 8, 32 # batch, tokens, token dimensionality\n",
    "x = torch.randn(B, T, C) # bs, ctx_len, token dimensionality\n",
    "\n",
    "# set up head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias = False)\n",
    "query = nn.Linear(C, head_size, bias = False)\n",
    "k = key(x) # (B, ctx_len, head_size) k: \"here is what I have\"\n",
    "q = query(x)# (B, ctx_len, head_size) q: \"here is my request/what I am interested in\"\n",
    "# each input token feature vector is used to create k,q.\n",
    "# each input token feature vector contains info on token identity and token position\n",
    "# so k,q are create wrt token identity and token position\n",
    "weights = k @ q.transpose(-2, -1) * head_size**-0.5 # k: (B, ctx_len, head_size) @  q: (B, head_size, ctx_len) --> (B, ctx_len, ctx_len) i.e. (B, T, T)\n",
    "# * head_size**-0.5: aka scaled attention. Idea: if weights not scaled, its variance is ~head_size -> when we apply softmax we might end up with a sharp pdist/~one-hot\n",
    "# which implies that we aggregate info from 1 single node/token which is bad: at init we want unormalized attention scores to be quite diffused\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# weights = torch.zeros((T,T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # put -inf in all positions where the tril has 0s\n",
    "# if we use an \"encoder\" block we delete the masking op here above: the idea is that eg if we want to do sentence classification (eg sentiment analysis) it\n",
    "# is not the case that we need to hide future tokens, cuz the algo \"works directly on the whole sentence\"\n",
    "# when we use the masking it is called a \"decoder\" block cuz is decoding language in this autoregressive manner\n",
    "\n",
    "# normalize attention scores\n",
    "weights = weights.softmax(-1) #F.softmax(weights, dim = -1) # softmax is always applied over the last dim\n",
    "\n",
    "# we don't aggregate directly raw x, but we get a version of x projected into a head_size dimensional space\n",
    "# out = weights @ x\n",
    "value = nn.Linear(C, head_size, bias = False)\n",
    "v = value(x) # v: \"here is what I communicate, my msg (if you find me interesting)\"\n",
    "out = weights @ v\n",
    "out.shape\n",
    "\n",
    "\n",
    "# you can thing to attention as a communication mechanism: you have N nodes and you can think it as a GNN aggregation step with particular weight matrix\n",
    "# attention is position/space agnosting, it's us that we provide positional info by summing pos_embeddings to the input (not like cnns that are space aware)\n",
    "\n",
    "# IMPO: the attention above is \"self-attention\" cuz the k,q,v are all coming from the same input x\n",
    "# cross-attention is when  we are comparing queries and values from 2 different inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5115, 0.4885, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2170, 0.5511, 0.2319, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2023, 0.2064, 0.2476, 0.3437, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1539, 0.2592, 0.1816, 0.2280, 0.1774, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2514, 0.1360, 0.1552, 0.1500, 0.1212, 0.1862, 0.0000, 0.0000],\n",
       "         [0.1071, 0.2490, 0.1200, 0.1561, 0.0963, 0.1064, 0.1651, 0.0000],\n",
       "         [0.1221, 0.1019, 0.0659, 0.1485, 0.1044, 0.1156, 0.1173, 0.2243]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6200, 0.3800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3634, 0.3304, 0.3062, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3164, 0.2543, 0.2079, 0.2213, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2759, 0.1713, 0.1802, 0.2785, 0.0942, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1189, 0.1927, 0.1843, 0.1472, 0.2079, 0.1491, 0.0000, 0.0000],\n",
       "         [0.1252, 0.1543, 0.1464, 0.1422, 0.1854, 0.1254, 0.1212, 0.0000],\n",
       "         [0.1520, 0.1062, 0.1163, 0.0733, 0.1224, 0.1178, 0.1148, 0.1973]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7224, 0.2776, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2640, 0.3768, 0.3592, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2258, 0.3877, 0.1309, 0.2556, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2037, 0.1491, 0.2948, 0.2022, 0.1503, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1105, 0.0980, 0.1654, 0.4310, 0.0768, 0.1182, 0.0000, 0.0000],\n",
       "         [0.1214, 0.1522, 0.0903, 0.1577, 0.1206, 0.2287, 0.1291, 0.0000],\n",
       "         [0.1075, 0.1289, 0.0725, 0.1480, 0.0940, 0.1141, 0.1047, 0.2303]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6175, 0.3825, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2036, 0.3841, 0.4123, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2430, 0.2184, 0.2755, 0.2632, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1864, 0.2851, 0.1135, 0.2544, 0.1606, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1364, 0.2083, 0.1257, 0.1605, 0.1515, 0.2176, 0.0000, 0.0000],\n",
       "         [0.1765, 0.1409, 0.0963, 0.1878, 0.1214, 0.1258, 0.1512, 0.0000],\n",
       "         [0.1741, 0.2116, 0.0369, 0.1585, 0.0653, 0.0692, 0.1232, 0.1611]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    '''\n",
    "    One single head of self attention\n",
    "    out_dims == head size\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_dims, out_dims, ctx_len):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(inp_dims, out_dims, bias=False)\n",
    "        self.query = nn.Linear(inp_dims, out_dims, bias=False)\n",
    "        self.value = nn.Linear(inp_dims, out_dims, bias=False)\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(ctx_len, ctx_len)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        # compute attention scores\n",
    "        weights = q @ k.transpose(-2,-1) * k.shape[-1] ** -0.5 # supposing k and q having the same dim\n",
    "        weights = weights.masked_fill(self.tril == 0, float('-inf')) # put -inf in all positions where the tril has 0s\n",
    "        weights = weights.softmax(-1)\n",
    "        weights = self.dropout(weights) # randomly prevent some node to communicate\n",
    "\n",
    "\n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = weights @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, T, C = 4, 8, 32 # batch, tokens, token dimensionality\n",
    "# x = torch.randn(B, T, C) # bs, ctx_len, token dimensionality\n",
    "# head = Head(C, 16, 8)\n",
    "# x = head(x)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    '''\n",
    "    The model learns each entry of a (vocab_len, vocab_len) table\n",
    "    where each entry is the probability dist of the following char given an input char at row\n",
    "    '''\n",
    "    def __init__(self, vocab_len, n_embed = 32):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (bigram model lecture 2)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_len, n_embed)\n",
    "\n",
    "        # let's add a position embedding table, the idea is that we also want to embed the position of each token (and not onty its identity as usually done by older language models)\n",
    "        self.position_embedding_table = nn.Embedding(ctx_len, n_embed) # across our whole max context length we have other n_embed vectors\n",
    "        self.self_att_head = Head(n_embed, n_embed, ctx_len)\n",
    "        self.lang_model_head = nn.Linear(n_embed, vocab_len)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape # T is int:ctx_len\n",
    "\n",
    "        # idx and targets are int tensors of shape (bs, ctx_len)\n",
    "        token_embeddings = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_embeddings = self.position_embedding_table(torch.arange(T)) # (T, n_embed)\n",
    "        x = token_embeddings + pos_embeddings # pos_embeddings broadcasted across batch dimension\n",
    "\n",
    "        x = self.self_att_head(x)\n",
    "\n",
    "        logits = self.lang_model_head(x) # (B, T, n_embed)\n",
    "        if targets == None: return logits, None\n",
    "\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    # this function will change over the course of the lecture\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (bs, ctx_len) array of int-idxs that define the context\n",
    "        # these int-idxs are chars from the vocab\n",
    "        # in the bigram model only 1 char is looked at as ctx\n",
    "        for _ in range(max_new_tokens):\n",
    "            # new: crop idx to be the last ctx_size token\n",
    "            idx_cond = idx[:, -ctx_len:]\n",
    "            # predict i.e. get unnormalized probs\n",
    "            logits, _ = self(idx_cond)\n",
    "            # get last time step\n",
    "            logits = logits[:, -1, :] # (bs, out_classes), -1 cuz rn we are using only the last char in the bigram model\n",
    "            # normalize probs\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (bs,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (bs, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.135059356689453"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_len=vocab_len)\n",
    "optim = torch.optim.AdamW(m.parameters(), lr=1e-3) # can't use larg lr with self-attention\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = m(xb, yb)\n",
    "loss.item() # we know that the initial loss  must be -math.log(1/vocab_len) = 4.174387269895637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated train loss: 4.185403823852539, estimated val loss: 4.187540054321289\n",
      "Estimated train loss: 2.7183759212493896, estimated val loss: 2.744436740875244\n",
      "Estimated train loss: 2.584773063659668, estimated val loss: 2.5600268840789795\n",
      "Estimated train loss: 2.5059614181518555, estimated val loss: 2.5117766857147217\n",
      "Estimated train loss: 2.5088212490081787, estimated val loss: 2.5001912117004395\n",
      "Estimated train loss: 2.46691632270813, estimated val loss: 2.466158390045166\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_num_steps = 5001 #25001\n",
    "eval_interval = 1000\n",
    "for step in range(max_num_steps):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if step % eval_interval == 0:\n",
    "        out = estimate_loss(m)\n",
    "        print(f\"Estimated train loss: {out['train']}, estimated val loss: {out['val']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Whipecelre.\n",
      "\n",
      "ARHENoceiens hsel'dlle do!\n",
      "AUSon.\n",
      "\n",
      "N:\n",
      "CESNGEGE:\n",
      "pes ingm?\n",
      "\n",
      "Iin:\n",
      "A tathe jghintnge:\n",
      "Cat int re mbriont;\n",
      "Th ce sourn!\n",
      "Fior vrithepergy othepid myot.\n",
      "\n",
      "Cye plro le do?\n",
      "Ou\n",
      "s,\n",
      "Sovore nism\n",
      "IAned I it ther\n",
      "Ang ach, bet oengdrendee, tchausich ongor fitof mworu, tot, V:\n",
      "Ant ind Firt homt; thed, nde, we\n",
      "AGUCOUKENGI's.\n",
      "\n",
      "SI,\n",
      "Whe, thepreer st alu hoven bee hse thinliche ingtwharl thirt ible,\n",
      "Ft!l sleds y.\n",
      "TOe snd tht id tature imbemeect yot:\n",
      "easreich ot br merle ce'ded\n",
      "INEses, the ng ku hanet edsme,\n",
      "An dong ar ith, to l;\n",
      "Yhave byer'ghe whest, at hteent,\n",
      "O orust am\n",
      "W;Yasto lod\n",
      "G epcr cors hafre nd.\n",
      "\n",
      "ARYe rdelepin!r th\n",
      "Igf.\n",
      "\n",
      "Ph, othevogth,\n",
      "I;\n",
      "I fepree o hoin me; o ghee nath it he chay bon chon mangis, tsenconcs Mut, met do teiowis tcthant\n",
      "J; st, frfe onte sa'stel baureeangourus'ce wot sheds in ar rod ghe che,\n",
      "I.\n",
      "\n",
      "Hol yot'o,\n",
      "Metay tid--t lhedl oor;ine thetathe the hougte ne wivett he wivencofowiny cfet lis iest hedl.\n",
      "\n",
      "Tonoe tarsailousaveu lininld DI iur oruters, r whund,\n",
      "S-t douncovepixrgtrow\n",
      "\n",
      "CAn fse?\n",
      "\n",
      "Nnot II lando ng steangdlt ist haclorit,\n",
      "Bo llinsestobathiutorve omedicy hel at sarlve lelyof wounown grAn titus of las, fiteldyeche ono ilsf nt!ay ingenceru. Theiwo haten btfothealy, yofrsat, gh felor sus, hanoche chebease tetors: hade.\n",
      "\n",
      "I worel sye hand cor'th ow on, the py pe yom thy onk;\n",
      "Sh, vachert at te\n",
      "A wher or yored yeeas ancerd ur fonoky:\n",
      "I it, ss prdeld thanowo he nt sidoreser shead dt fotr, hyond RHrM orus\n",
      "Wheasesifthe cead binentt,\n",
      "MOH ees yore'r-:\n",
      "Hhen gher, it chithixree fm uest bdy yk to beay is tharerruburet, turt ml, pen otus ct,\n",
      "F worwive;\n",
      "Y hore cheing pronee cet, gikenatllolfrnor finu dganvaspofrhe\n",
      "Theen.\n",
      "Hurester.Yowhes be.\n",
      "\n",
      "LAle hy ancake hover oulseedat.\n",
      "\n",
      "Gow GLAt fangt ty otnst st oun wis fpre koond ads endad iculre ond thord:\n",
      "LO:\n",
      "OAr nariso m? Rileat otonrivak.\n",
      "\n",
      "Han.\n",
      "Yicher hse ly irk wier, uscher wer tONI\n",
      "Amise Y be sy louregt I sithpead te, woruse,'Y\n",
      "Angnt thel to heomes th, fod, tl tth, fm che l shad outry antis orse yowit wong hy anugen: ine\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 8), dtype=torch.long)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead attention\n",
    "## return to NB ~1:16\n",
    "Apply multiple attentions in parallel and concatenating the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''out_dims == head size'''\n",
    "    def __init__(self, inp_dims, out_dims, ctx_len, heads_number):\n",
    "        super().__init__()\n",
    "        self.heads = [Head(inp_dims, out_dims, ctx_len) for _ in range(heads_number)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # concatenate over the last dimension, so we get a feature vector that is:\n",
    "        # (bs, ctx_len, out_dims*heads_number)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# think as each self attention as a communication channel, now with N self-attention heads we have N number of communication channel\n",
    "# given that you concat the outputs usually what you do is that if the initial feature vector size is eg 32 then 32//number of heads eg 4 32/4= 8 then 8 is head size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, T, C = 4, 8, 32 # batch, ctx_len, token dimensionality\n",
    "# x = torch.randn(B, T, C) # bs, ctx_len, token dimensionality\n",
    "# multi_head = MultiHeadAttention(C, 16, 8, 15)\n",
    "# x = multi_head(x)\n",
    "# x.shape # torch.Size([4, 8, 240]): 16 * 15 = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    '''\n",
    "    The model learns each entry of a (vocab_len, vocab_len) table\n",
    "    where each entry is the probability dist of the following char given an input char at row\n",
    "    '''\n",
    "    def __init__(self, vocab_len, n_embed = 32):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (bigram model lecture 2)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_len, n_embed)\n",
    "\n",
    "        # let's add a position embedding table, the idea is that we also want to embed the position of each token (and not onty its identity as usually done by older language models)\n",
    "        self.position_embedding_table = nn.Embedding(ctx_len, n_embed) # across our whole max context length we have other n_embed vectors\n",
    "        self.self_att_head = MultiHeadAttention(inp_dims=n_embed, out_dims=8, ctx_len=8, heads_number=4)\n",
    "        self.lang_model_head = nn.Linear(n_embed, vocab_len)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape # T is int:ctx_len\n",
    "\n",
    "        # idx and targets are int tensors of shape (bs, ctx_len)\n",
    "        token_embeddings = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_embeddings = self.position_embedding_table(torch.arange(T)) # (T, n_embed)\n",
    "        x = token_embeddings + pos_embeddings # pos_embeddings broadcasted across batch dimension\n",
    "        x = self.self_att_head(x)\n",
    "        logits = self.lang_model_head(x) # (B, T, n_embed)\n",
    "        if targets == None: return logits, None\n",
    "\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    # this function will change over the course of the lecture\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (bs, ctx_len) array of int-idxs that define the context\n",
    "        # these int-idxs are chars from the vocab\n",
    "        # in the bigram model only 1 char is looked at as ctx\n",
    "        for _ in range(max_new_tokens):\n",
    "            # new: crop idx to be the last ctx_size token\n",
    "            idx_cond = idx[:, -ctx_len:]\n",
    "            # predict i.e. get unnormalized probs\n",
    "            logits, _ = self(idx_cond)\n",
    "            # get last time step\n",
    "            logits = logits[:, -1, :] # (bs, out_classes), -1 cuz rn we are using only the last char in the bigram model\n",
    "            # normalize probs\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (bs,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (bs, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated train loss: 4.2972731590271, estimated val loss: 4.2917327880859375\n",
      "Estimated train loss: 3.091881036758423, estimated val loss: 3.1385698318481445\n",
      "Estimated train loss: 2.9332191944122314, estimated val loss: 2.9581198692321777\n",
      "Estimated train loss: 2.809567451477051, estimated val loss: 2.858900785446167\n",
      "Estimated train loss: 2.742405414581299, estimated val loss: 2.7311315536499023\n",
      "Estimated train loss: 2.6685190200805664, estimated val loss: 2.6992881298065186\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_len=vocab_len)\n",
    "optim = torch.optim.AdamW(m.parameters(), lr=1e-3) # can't use larg lr with self-attention\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = m(xb, yb)\n",
    "loss.item() # we know that the initial loss  must be -math.log(1/vocab_len) = 4.174387269895637\n",
    "\n",
    "batch_size = 32\n",
    "max_num_steps = 5001 #25001\n",
    "eval_interval = 1000\n",
    "for step in range(max_num_steps):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if step % eval_interval == 0:\n",
    "        out = estimate_loss(m)\n",
    "        print(f\"Estimated train loss: {out['train']}, estimated val loss: {out['val']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TIadKsge roownocusing sgdo rAncie sy\n",
      "as se uteyom fduaud snote'ta lhe yorf?ithy\n",
      "\n",
      "eceg ssi-os\n",
      "Mrstve at crkacy Vo rposof onreruseernteie!ouif ta e\n",
      "irlrwaha 'afaivhhouse l mrosisan ss fow oBere cas.\n",
      "nr terhiact nat sir..\n",
      "hn; onod:e Merouenyp he py,,pm:\n",
      "Stis,U\n",
      "CIAdC tcse mhua gh natstodee e fost,\n",
      "I ny\n",
      "Ln tosheith aoe, aco thy i to,en ba L afnavsin r th ani-e\n",
      "vil\n",
      "Anry wie, f, oh thopolok dl Cheidl y amn nh tonta Ianyd\n",
      "Wi pyBoansem\n",
      " br wpea n m ntilrogrriitWeenr\n",
      "usgs th, thros hi nenfirtae\n",
      "a pmuandetse lho th\n",
      "G alBh.\n",
      "I\n",
      "Igs\n",
      "Lnd arheBn firesy cy ivho arw mirtteay les\n",
      "pa awerlif-r ameosoruss pt-nc wh y owr: gsaate. phsThe;e, Ia wr fivehye cal.\n",
      "e\n",
      "IerAi mT yaryhe bpes feloH thaaf, t,\n",
      "n nryoted seurerer nscco run meVJ Wsopet woirt\n",
      "IoI\n",
      "KThosheewaUarimta w,hisIl dnoandad steh ha torittise ms\n",
      "R w anosaddr ti cenreg Cadanasd,A tin! y\n",
      "Fh iPkar\n",
      "THohnqlte, m as ursntn, R\n",
      "Acg malpoY iirted notan ty peoRnal sd\n",
      "ecrEerMar Ceercianwet ff tedee-ttr l\n",
      "ATfran t thaseares Isic, te ofalin thernathenKonicax hee spearn, runcicoT mosrrealt su,alhr bed;ee bleproreon rItdgor e hadasd\n",
      "runs worid, d prin tnobhee slu fn thaavares u?ep s limithk bto-ot,o, utant st!omoSue acfrese tor thoul.\n",
      "R ip Rader\n",
      "OTrocivlerar\n",
      "j; pornginy:\n",
      "h-irhime t trahoren r,mda,\n",
      "o bacnsembe t he sre t,eleol t lribsis n w tgenn pes,es,Ay\n",
      "\n",
      "oo ay'WrdF ri lGereren angetr-n\n",
      "par,h ad, iosde asI' soy.a.\n",
      "SaN\n",
      "Ul'Tsy,in t psehare seur hisessenalhsir-se in'dsod-e s hoime beaswasi sit,,h\n",
      ",F supake,ucttel k! oh po flr llousutg e g inciti weire?, be.\n",
      "\n",
      "Fsnx De wapve fy fi thit;O wer ovore yi tCmifn ae,df cot hahugdoser G.\n",
      "LUIMCia kforgotayi\n",
      "\n",
      "Tr?oede s faasrent' l co awis noirts n bue semad mtpohor ks ws,iIor pt saco he atfer s'yhe oiat roas ifh tylino w Cosex to sherw e thtranne iratlcine; fGuvar h tsar atyowne, Iee sedt tbaou fe woncoacI h y mhifefo Lluchrofo Mutidit;es bos fo'Trram Ie tonfpoyl\n",
      "Oh:m l asoapundodual!, riinn te\n",
      "ha;a.\n",
      "DMen?:\n",
      "E inilSw:\n",
      "A fly\n",
      "I'hesurs l owrouroh Ssdor asla ates at teshaola ap. Pehtjay mar: litolvwae welidrugitl s \n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 8), dtype=torch.long)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, inp_dims):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(inp_dims, inp_dims),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    '''\n",
    "    The model learns each entry of a (vocab_len, vocab_len) table\n",
    "    where each entry is the probability dist of the following char given an input char at row\n",
    "    '''\n",
    "    def __init__(self, vocab_len, n_embed = 32):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (bigram model lecture 2)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_len, n_embed)\n",
    "\n",
    "        # let's add a position embedding table, the idea is that we also want to embed the position of each token (and not onty its identity as usually done by older language models)\n",
    "        self.position_embedding_table = nn.Embedding(ctx_len, n_embed) # across our whole max context length we have other n_embed vectors\n",
    "        self.self_att_head = MultiHeadAttention(inp_dims=n_embed, out_dims=8, ctx_len=8, heads_number=4)\n",
    "        self.mlp = FeedForward(n_embed) # impo the mlp embeds/works at char/token lvl\n",
    "        self.lang_model_head = nn.Linear(n_embed, vocab_len)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape # T is int:ctx_len\n",
    "\n",
    "        # idx and targets are int tensors of shape (bs, ctx_len)\n",
    "        token_embeddings = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_embeddings = self.position_embedding_table(torch.arange(T)) # (T, n_embed)\n",
    "        x = token_embeddings + pos_embeddings # pos_embeddings broadcasted across batch dimension\n",
    "\n",
    "        # communication\n",
    "        x = self.self_att_head(x)\n",
    "\n",
    "        # computation\n",
    "        x = self.mlp(x)\n",
    "        logits = self.lang_model_head(x) # (B, T, n_embed)\n",
    "        if targets == None: return logits, None\n",
    "\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    # this function will change over the course of the lecture\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (bs, ctx_len) array of int-idxs that define the context\n",
    "        # these int-idxs are chars from the vocab\n",
    "        # in the bigram model only 1 char is looked at as ctx\n",
    "        for _ in range(max_new_tokens):\n",
    "            # new: crop idx to be the last ctx_size token\n",
    "            idx_cond = idx[:, -ctx_len:]\n",
    "            # predict i.e. get unnormalized probs\n",
    "            logits, _ = self(idx_cond)\n",
    "            # get last time step\n",
    "            logits = logits[:, -1, :] # (bs, out_classes), -1 cuz rn we are using only the last char in the bigram model\n",
    "            # normalize probs\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (bs,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (bs, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = BigramLanguageModel(vocab_len=vocab_len)\n",
    "# optim = torch.optim.AdamW(m.parameters(), lr=1e-3) # can't use larg lr with self-attention\n",
    "# xb, yb = get_batch('train')\n",
    "# logits, loss = m(xb, yb)\n",
    "# loss.item() # we know that the initial loss  must be -math.log(1/vocab_len) = 4.174387269895637\n",
    "\n",
    "# batch_size = 32\n",
    "# max_num_steps = 5001 #25001\n",
    "# eval_interval = 1000\n",
    "# for step in range(max_num_steps):\n",
    "#     xb, yb = get_batch('train')\n",
    "#     logits, loss = m(xb, yb)\n",
    "#     optim.zero_grad(set_to_none=True)\n",
    "#     loss.backward()\n",
    "#     optim.step()\n",
    "#     if step % eval_interval == 0:\n",
    "#         out = estimate_loss(m)\n",
    "#         print(f\"Estimated train loss: {out['train']}, estimated val loss: {out['val']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OTCIyADTON\n",
      "TShaminosd\n",
      "Mulit!H\n",
      "s fodr ueividat ti ou wos amo?\n",
      " nwlonr hthifets hitare bsthaipsl\n",
      "won nto um pow ti os\n",
      "Al rfe akeead lndu.\n",
      "\n",
      "MLD\n",
      "NYI::\n",
      "W:\n",
      "T-OLhu\n",
      "ThA GlS:\n",
      "TLsid\n",
      "yotedn pswid eiy;\n",
      "Anmn wh pami', yre bea ca wth Ie.\n",
      "\n",
      "INLI\n",
      "V:\n",
      "cJTEhe; ilrs\n",
      "Thricsalmy:\n",
      "Wroat dle sasoulde ts a' Ie at'lnle thsad!\n",
      "WI m se,enans\n",
      "MAieueged thene scoutk, or fo hopencce 'd abi pyo gfe ms.\n",
      "\n",
      "\n",
      "TECU:\n",
      "WThey\n",
      "IBDoul ond'fudlss tath.Sle\n",
      "Mo isrMomens. t ths\n",
      "AW\n",
      "LLIIO.Wolr b sor:\n",
      " drHminge elalsesur,\n",
      "ofr ke bbeyawe't ti houi fo dlrteu.\n",
      "\n",
      "\n",
      "LATDNserA\n",
      "hDrou Lhifny hime ntlse afi cossulom msoomsun fgod the\n",
      ": ne tshont I qt kepr las thomoawoml thil,lumt moul, tee corfrtut vampelo ptf It ai hadlansad.\n",
      "\n",
      "GSISSTeehr'gors\n",
      "o thitmheen doddd ga fenavon.\n",
      "\n",
      "MIH wICWopia shleevala tes dnlale t hoerlacpan;\n",
      "TMe fottof ure thereche m lar.\n",
      "\n",
      "\n",
      "isiv istl ter, the.\n",
      "\n",
      "E\n",
      "TUhSAams to msouk hher.\n",
      "\n",
      "A thaker:\n",
      "\n",
      "TEhawersl wread wt oxnors vy so wtan? VOn a mnlr ususdl loetrolrualr'de aclr mehivee sli, at qith afeot saqtrton ml!\n",
      "I:\n",
      "\n",
      "RhAUUNTVROLDu\n",
      "YHOAe:\n",
      "\n",
      "Auinsa etts or ber' mhyl, alis s!cth\n",
      "wivre Yo Iloulmam hotanmen t opyk mwlw! at, weuhee matu fords bR e thv afere,s\n",
      "A lnl af gfeerl opleigennencl!e fenarr,, govl Riuut igtheodts touv baacwhomns\n",
      "Ebr! aphornd wu,,\n",
      "LLingo I\n",
      "Wror thig noe tonsd  yl tacIT,hea bvratet w th irvo fe mcyhel,i!\n",
      "\n",
      "A et.IO\n",
      "EYThot. EI:\n",
      "WelLowe d errovs ous tes l che wthinsem thaghouw tod tirounott we aae agbe. ac's,\n",
      "Shot':\n",
      "S, wo t bethoj ghe;,n wersovee asl e hes tor.\n",
      "\n",
      "LOoesUMIy thoadnte hus issr contulou god dd ety umul,!idn inslrricitn toulo k de h carle; wrrosk-s, dnei!\n",
      "FA\n",
      "WAYerik\n",
      "Ai bq wwan dunds giaferrcist, ut,\n",
      "annau th biycl le houv atavocran se thghhssee aitusvaav.\n",
      "\n",
      "TIr\n",
      "i\n",
      "ROAU\n",
      "SEDinnse mtcealedn thde foon hsow,otc: l afeimoerd th tmhs; blne\n",
      "Wedoenntm th ghb whe hiparsnsimo,  be byugin me wismirtan the be k sous ed\n",
      "Thhiose.\n",
      "\n",
      "BLHKIO\n",
      "TCIRormTnpch,' lthe Ileaw i idund d cfaE\n",
      "owL:\n",
      "UO rbU.\n",
      "TaaclOosr, er th!\n",
      "ot wooun bt coenn.\n",
      "\n",
      "\n",
      " hit,e be mlpad ol b-f ac difro tirbemle ms ti dhehend.\n",
      "Wo nd\n",
      "H heuslO\n",
      ":\n",
      "LCERTAVYO\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 8), dtype=torch.long)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''out_dims == head size'''\n",
    "    def __init__(self, inp_dims, out_dims, ctx_len, heads_number):\n",
    "        super().__init__()\n",
    "        self.heads = [Head(inp_dims, out_dims, ctx_len) for _ in range(heads_number)]\n",
    "        self.projection = nn.Linear(inp_dims, inp_dims) # go back to input dims s.t. apply residual path\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # concatenate over the last dimension, so we get a feature vector that is:\n",
    "        # (bs, ctx_len, out_dims*heads_number)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.projection(out) # projection back into the residual pathway\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, inp_dims):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(inp_dims, 4*inp_dims), # 4* as used in the paper\n",
    "            nn.ReLU(),\n",
    "            # projection layer\n",
    "            nn.Linear(4*inp_dims, inp_dims) # projection back into the residual pathway\n",
    "            nn.Dropout(.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class LayerNorm():\n",
    "    '''\n",
    "    BN made sure that across the whole batch each neuron output was ~N(0,1)\n",
    "    bn was normalizing over the cols,\n",
    "    layer norm normalizes over the rows -> indipendent across batches\n",
    "    '''\n",
    "    def __init__(self, dims, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        # params\n",
    "        self.gamma = torch.ones(dims)\n",
    "        self.beta = torch.zeros(dims)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # the batch and the ctxt act as batch dimensions -> not treated in the computation\n",
    "        # per token transformation\n",
    "        xmean = x.mean(1, keepdim=True)\n",
    "        xvar = x.var(1, keepdim=True, unbiased=True)\n",
    "        xhat = (x - xmean)/ torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self): return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "# think as each self attention as a communication channel, now with N self-attention heads we have N number of communication channel\n",
    "# given that you concat the outputs usually what you do is that if the initial feature vector size is eg 32 then 32//number of heads eg 4 32/4= 8 then 8 is head size\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''\n",
    "    A core transformer block, it implements attention (communication step), and computation as form of mlp\n",
    "    the mlp is applied to each token\n",
    "    this block is repeated N times in a transformer -> norm layers + residual connections are thus required\n",
    "    '''\n",
    "    def __init__(self, n_embed, heads_number, ctx_len):\n",
    "        super().__init__()\n",
    "        head_size = n_embed//heads_number\n",
    "        self.multi_head_attention =  MultiHeadAttention(inp_dims=n_embed, out_dims=head_size, ctx_len=ctx_len, heads_number=heads_number)\n",
    "        self.mlp = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # the x + ... is the branching/residual connections\n",
    "        # given that the + op distributes the gradient equivalently, all the gradient that is coming from the loss is forked to:\n",
    "        # 1) the params of self.multi_head_attention(x)\n",
    "        # 2) directly to the next layer thru x\n",
    "        # this implies that in the beginning of the training, when we have gradients that need to\n",
    "\n",
    "        # the actual block (multi_head_attention + mlp) at the beginning of the trainig are initialized s.t. make them \"inactive\" and\n",
    "        # they come online during training (by their \"normal\" weights updates) but this way in the beginning we have \"pure\" gradient that goes from out to inp\n",
    "        # which allows each block to recieve a good ammount of gradient\n",
    "        # note that to use residual connections we need also to modify MultiHeadAttention to make it support linear projections for in/out size matching\n",
    "        x = x + self.multi_head_attention(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 32 # batch, ctx_len, token dimensionality\n",
    "x = torch.randn(B, T, C) # bs, ctx_len, token dimensionality\n",
    "block = Block(32, 4, 8)\n",
    "x = block(x)\n",
    "x.shape # torch.Size([4, 8, 240]): 16 * 15 = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    '''\n",
    "    The model learns each entry of a (vocab_len, vocab_len) table\n",
    "    where each entry is the probability dist of the following char given an input char at row\n",
    "    '''\n",
    "    def __init__(self, vocab_len, n_embed = 32):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (bigram model lecture 2)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_len, n_embed)\n",
    "\n",
    "        # let's add a position embedding table, the idea is that we also want to embed the position of each token (and not onty its identity as usually done by older language models)\n",
    "        self.position_embedding_table = nn.Embedding(ctx_len, n_embed) # across our whole max context length we have other n_embed vectors\n",
    "\n",
    "        # self.self_att_head = MultiHeadAttention(inp_dims=n_embed, out_dims=8, ctx_len=8, heads_number=4)\n",
    "        # self.mlp = FeedForward(n_embed) # impo the mlp embeds/works at char/token lvl\n",
    "        n_layer = 4 # number of attention/mlp or communication/computation blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, heads_number=4, ctx_len=ctx_len) for _ in range(n_layer)])\n",
    "        self.blocks.append(nn.LayerNorm(n_embed))\n",
    "\n",
    "        self.lang_model_head = nn.Linear(n_embed, vocab_len)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape # T is int:ctx_len\n",
    "\n",
    "        # idx and targets are int tensors of shape (bs, ctx_len)\n",
    "        token_embeddings = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_embeddings = self.position_embedding_table(torch.arange(T)) # (T, n_embed)\n",
    "        x = token_embeddings + pos_embeddings # pos_embeddings broadcasted across batch dimension\n",
    "\n",
    "        x = self.blocks(x) # but now with this we have a deep net so we need residual connections and layer normalization as in the og paper\n",
    "\n",
    "        logits = self.lang_model_head(x) # (B, T, n_embed)\n",
    "        if targets == None: return logits, None\n",
    "\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    # this function will change over the course of the lecture\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (bs, ctx_len) array of int-idxs that define the context\n",
    "        # these int-idxs are chars from the vocab\n",
    "        # in the bigram model only 1 char is looked at as ctx\n",
    "        for _ in range(max_new_tokens):\n",
    "            # new: crop idx to be the last ctx_size token\n",
    "            idx_cond = idx[:, -ctx_len:]\n",
    "            # predict i.e. get unnormalized probs\n",
    "            logits, _ = self(idx_cond)\n",
    "            # get last time step\n",
    "            logits = logits[:, -1, :] # (bs, out_classes), -1 cuz rn we are using only the last char in the bigram model\n",
    "            # normalize probs\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (bs,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (bs, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated train loss: 2.3352577686309814, estimated val loss: 2.3268487453460693\n",
      "Estimated train loss: 2.2995352745056152, estimated val loss: 2.30171275138855\n",
      "Estimated train loss: 2.268794298171997, estimated val loss: 2.3098061084747314\n",
      "Estimated train loss: 2.249471664428711, estimated val loss: 2.2297983169555664\n",
      "Estimated train loss: 2.2284390926361084, estimated val loss: 2.2711191177368164\n",
      "Estimated train loss: 2.2322747707366943, estimated val loss: 2.2687439918518066\n",
      "Estimated train loss: 2.208101987838745, estimated val loss: 2.215780019760132\n",
      "Estimated train loss: 2.1785662174224854, estimated val loss: 2.237980842590332\n",
      "Estimated train loss: 2.1790497303009033, estimated val loss: 2.238198757171631\n",
      "Estimated train loss: 2.1921520233154297, estimated val loss: 2.219381332397461\n",
      "Estimated train loss: 2.1828908920288086, estimated val loss: 2.2044055461883545\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_len=vocab_len)\n",
    "optim = torch.optim.AdamW(m.parameters(), lr=3e-4) # can't use larg lr with self-attention\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = m(xb, yb)\n",
    "loss.item() # we know that the initial loss  must be -math.log(1/vocab_len) = 4.174387269895637\n",
    "\n",
    "batch_size = 64\n",
    "max_num_steps = 5001 #25001\n",
    "eval_interval = 1000\n",
    "for step in range(max_num_steps):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if step % eval_interval == 0:\n",
    "        out = estimate_loss(m)\n",
    "        print(f\"Estimated train loss: {out['train']}, estimated val loss: {out['val']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LAONGES:\n",
      "Cact exhplired as itwo, lor much\n",
      "hen ho is to tway\n",
      "Sath then he thou that that thad it thou,\n",
      "Thine alvit an theh would: what falll thath the parkse,\n",
      "Is hill wepting at sphat; chart you, dor gae beshtle oupomg yont, fall is wet nomp.\n",
      "\n",
      "SRICK:\n",
      "Nut mat stay, yis swhere un muce that tot,\n",
      "bits the this bay\n",
      "Th raimpt the's midling,\n",
      "Ne oxs wances wearh sine youste gokin\n",
      "Snoul the Ralie thouse beacting noot.\n",
      "Mity,\n",
      "Ink wonet sup eve gavitll as arer faliosst, like,\n",
      "Whit afe noll feueth suty vith nom.\n",
      "\n",
      "\n",
      "RLIIO:\n",
      "I in that 'lave lave by obee gant he o's mow the will oss hid, Out I yoll to i nimste coulent.\n",
      "\n",
      "DOMELFO::\n",
      "Sip chote\n",
      "ive sau mpon\n",
      "he phe shatf hall save now but hak surde aghe.\n",
      "\n",
      "De ISTIND:\n",
      "Mak high thou agh five a re lamest yourld onth thal bat ses surk my awrer sowe,\n",
      "Hingh nimakith,\n",
      "Ans te yom my slight in lot! come hexsoul\n",
      "Sarllgean!\n",
      "OF IVF MABENTIA:\n",
      "Nraak there my jutction.\n",
      "The Mut is mallus tot: my ame, he's de comme\n",
      "Mell ire sere;\n",
      "That I brod:\n",
      "And tobosele Siste cwild whe tike that fon tach visth, faid ins Wee worll by thachgue fivest, he me wouth wit donde thee,\n",
      "Me salllg.\n",
      "\n",
      "SConand,\n",
      "Bued fall and pasie, sphatl shave by couont,\n",
      "how whis Campen hall, gond that madst gouste that think weay have nomak be thive go as that, Theaw, en the, lod.\n",
      "\n",
      "HAFtind therd thos the pow thould,\n",
      "Sind ital that elat you, limot.\n",
      "\n",
      "TRONCE:\n",
      "A gonght,\n",
      "Hent. IF Barily!\n",
      "Fe Chay tastigh\n",
      "Pad thou somPre\n",
      "To thaver, you; sid wet suth ame tath luts, fot a since:\n",
      "What tho adLH\n",
      "Weand the, Is\n",
      "\n",
      "KIDOR dagttie mucth prichmin walde the a kewUaton that heald: vis dive ening:\n",
      "I hilbe howh, the us liests, ean frireh, bell,\n",
      "IClance,\n",
      "Thif higghl, I wentigh!\n",
      "Fir fatty rath he basenty, ou win donsh. and that tip fivir I hourket.\n",
      "Cor sheau?\n",
      "I Clod, If what's tigh, a Rone: i ron thip. Vo Rmautione,\n",
      "Aperesrtond as soll hen the mars\n",
      "Shot our bure fin noleg,\n",
      "By, Juter but-dan pip dis that shy\n",
      "That his tak bond.\n",
      "golt aind with, shinis ahae,\n",
      "Ibe If lare theme ming tha bratoncer sheb live selll\n",
      "I nibketat me the wA\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 8), dtype=torch.long)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathyAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

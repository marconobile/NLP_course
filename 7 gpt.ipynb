{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data and Preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-06 00:21:04--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.3’\n",
      "\n",
      "input.txt.3         100%[===================>]   1.06M  1.12MB/s    in 1.0s    \n",
      "\n",
      "2024-01-06 00:21:06 (1.12 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download data:\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len of data i.e. num of chars\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 1k chars\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# let's get vocab\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_len = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64} {0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "[46, 43, 50, 50, 53, 61, 1, 61, 53, 56, 50, 42, 2, 4, 2]\n",
      "hellow world!&!\n"
     ]
    }
   ],
   "source": [
    "# let's tokenize text at the char level\n",
    "# build mappings\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for ch,i in stoi.items()}\n",
    "print(stoi, itos)\n",
    "\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode('hellow world!&!'))\n",
    "print(decode(encode('hellow world!&!')))\n",
    "\n",
    "# there are many tokenization schemes eg google uses SentencePiece (sub-word tokenizer), openAI uses tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so now we can tokenize the input corpus\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's split data in train/test/val\n",
    "n = int(.9*len(data))\n",
    "train_data = data[:n] # 90%\n",
    "val_data = data[n:] # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_len = 8\n",
    "train_data[:ctx_len+1] # a first example of input data\n",
    "# here we have that 47 comes after 18, 56 comes after 18 and 47, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:ctx_len]\n",
    "y = train_data[1:ctx_len+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18, 47, 56, 57, 58,  1, 15, 47]),\n",
       " tensor([47, 56, 57, 58,  1, 15, 47, 58]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Context: tensor([18]), target: 47\n",
      "Sample 1, Context: tensor([18, 47]), target: 56\n",
      "Sample 2, Context: tensor([18, 47, 56]), target: 57\n",
      "Sample 3, Context: tensor([18, 47, 56, 57]), target: 58\n",
      "Sample 4, Context: tensor([18, 47, 56, 57, 58]), target: 1\n",
      "Sample 5, Context: tensor([18, 47, 56, 57, 58,  1]), target: 15\n",
      "Sample 6, Context: tensor([18, 47, 56, 57, 58,  1, 15]), target: 47\n",
      "Sample 7, Context: tensor([18, 47, 56, 57, 58,  1, 15, 47]), target: 58\n"
     ]
    }
   ],
   "source": [
    "for t in range(ctx_len):\n",
    "    ctx = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Sample {t}, Context: {ctx}, target: {target}\") # so given a single chunk of the train data within a contex block we have 8 samples\n",
    "    # it is important to train with all data with context between 1 and ctx_size cuz transformer must be able to adapt to any input size\n",
    "    # thus we wrap up all these samples in a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[32, 46, 43, 47, 56,  1, 61, 39],\n",
       "         [ 1, 19, 13, 33, 26, 32, 10,  0],\n",
       "         [63,  1, 46, 53, 52, 43, 57, 58],\n",
       "         [58, 47, 44, 63,  1, 58, 46, 47]]),\n",
       " tensor([[46, 43, 47, 56,  1, 61, 39, 58],\n",
       "         [19, 13, 33, 26, 32, 10,  0, 32],\n",
       "         [ 1, 46, 53, 52, 43, 57, 58, 63],\n",
       "         [47, 44, 63,  1, 58, 46, 47, 57]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_data\n",
    "ix = torch.randint(len(data) - ctx_len, (4,)) # up to last char - ctx block s.t. have a complete block even at the end of dataset\n",
    "x = torch.stack([data[i:i+ctx_len]  for i in ix])\n",
    "y = torch.stack([data[i+1:i+ctx_len+1]  for i in ix])\n",
    "x, y \n",
    "\n",
    "# so we here we have 32 samples (bs * ctx_len) cuz for each (x[i, 0:j] for j from 0 to ctx_len) we have a yij (look above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "bs = 4 \n",
    "ctx_len = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - ctx_len, (bs,)) # up to last char - ctx block s.t. have a complete block even at the end of dataset\n",
    "    x = torch.stack([data[i:i+ctx_len]  for i in ix])\n",
    "    y = torch.stack([data[i+1:i+ctx_len+1]  for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            losses[i] = loss\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: BigramLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    '''\n",
    "    The model learns each entry of a (vocab_len, vocab_len) table \n",
    "    where each entry is the probability dist of the following char given an input char at row\n",
    "    '''\n",
    "    def __init__(self, vocab_len):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (bigram model lecture 2)    \n",
    "        self.token_embedding_table = nn.Embedding(vocab_len, vocab_len)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are int tensors of shape (bs, ctx_len)\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets == None: return logits, None        \n",
    "            \n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)                \n",
    "        return logits, loss\n",
    "    \n",
    "    # this function will change over the course of the lecture\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (bs, ctx_len) array of int-idxs that define the context\n",
    "        # these int-idxs are chars from the vocab\n",
    "        # in the bigram model only 1 char is looked at as ctx\n",
    "        for _ in range(max_new_tokens):\n",
    "            # predict i.e. get unnormalized probs\n",
    "            logits, _ = self(idx)\n",
    "            # get last time step\n",
    "            logits = logits[:, -1, :] # (bs, out_classes), -1 cuz rn we are using only the last char in the bigram model\n",
    "            # normalize probs\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (bs,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (bs, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
       "         [44, 53, 56,  1, 58, 46, 39, 58],\n",
       "         [52, 58,  1, 58, 46, 39, 58,  1],\n",
       "         [25, 17, 27, 10,  0, 21,  1, 54]]),\n",
       " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
       "         [53, 56,  1, 58, 46, 39, 58,  1],\n",
       "         [58,  1, 58, 46, 39, 58,  1, 46],\n",
       "         [17, 27, 10,  0, 21,  1, 54, 39]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, yb.shape)\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.677961826324463"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_len=vocab_len)\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = m(xb, yb)\n",
    "logits.shape # for each chunk of text selected (bs = 4) of size ctx_len (8), we deconstruct the text in a sequential manner s.t. create\n",
    "# 8 samples so for each one of the 8*4=32 samples we get a vocab_size tensor that represent the prob dist of over the next char\n",
    "# all of these given that we are using directly embeddings is just as indexing into the token_embedding_table \n",
    "\n",
    "loss.item() # we know that the initial loss  must be -math.log(1/vocab_len) = 4.174387269895637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nkrENNTjLDuQcLzy'RIo;'KdhpV\\nvLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq!Lzn\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long) # torch.long = int; 0 is \\n so good char to begin generation\n",
    "decode(m.generate(idx, max_new_tokens=100)[0].tolist())\n",
    "# atm trash cuz not trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marconobile/miniconda3/envs/karpathyAI/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400366987/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated train loss: 4.641025066375732, estimated val loss: 4.660730361938477\n",
      "Estimated train loss: 2.822432041168213, estimated val loss: 2.850165367126465\n",
      "Estimated train loss: 2.5394654273986816, estimated val loss: 2.5837109088897705\n",
      "Estimated train loss: 2.504521369934082, estimated val loss: 2.495387315750122\n",
      "Estimated train loss: 2.48500394821167, estimated val loss: 2.5055902004241943\n",
      "Estimated train loss: 2.4403717517852783, estimated val loss: 2.4746057987213135\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_num_steps = 25001\n",
    "eval_interval = 5000\n",
    "for step in range(max_num_steps):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if step % eval_interval == 0:\n",
    "        out = estimate_loss(m)\n",
    "        print(f\"Estimated train loss: {out['train']}, estimated val loss: {out['val']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nBUnsist w; miome!\\nGQUps anomahall wherince ithity, st: ginodishelodeas s hengofrof S:\\n3Be topof qulcadusullowompr Lein I schivefio te aine sther tho Apl\\nAD otoese s MPe '?\\nWig paiceneelin g se?\\nOMELid y, p't ineay epevend me,\\nOur oulel yo n at, fef und 'Whaithe thoounthasindstre ge spld my\\npre t ge\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "decode(m.generate(idx, max_new_tokens=300)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that we want chars/tokens to talk to each other to generate a meaningful context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now see an important mathematical trick at the hearth of __self-attention__ implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2 # batch, tokens, token dimensionality\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing to notice is that context can flow only flow from past up untill current token (we are trying to predict next word, we cannot use it as context). Thus context for current token is only retrospective.\n",
    "A naive approach would be to take an average of current token and all feature vectors of previously processed tokens, s.t. get a form or retrospective context. Naive cuz we loose positionality, ordering and we take all info as equal. \n",
    "For now let's implement this naive average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want x[B, T] = mean_{i<=t>} x[B, i]\n",
    "xbow = torch.zeros(B, T, C) # C token dimensionality\n",
    "for b in range(B): # for each obs in the batch\n",
    "    for t in range(T): # for each token in obs\n",
    "        xprev = x[b, :t+1] # select the given obs, up untill current token, current included, (t, C)\n",
    "        xbow[b,t] = xprev.mean(0) # take avg of selected tokens, go next token\n",
    "# recall here that if we have\n",
    "# [\n",
    "#     x1: [x11, x12],\n",
    "#     x2: [x21, x22],\n",
    "#     x3: [x31, x32],\n",
    "#     x4: [x41, x42],\n",
    "#     x5: [x51, x52]\n",
    "# ]\n",
    "        \n",
    "# then each i-th row of the resulting matrix is the col-wise average up until the i-th row of the data matrix\n",
    "# [\n",
    "#     avg(x1):             [x11, x12]/1,\n",
    "#     avg(x1,x2):          [x11+x21, x12+x22]/2, \n",
    "#     avg(x1,x2,x3):       [x11+x21+x31, x12+x22+x32]/3,\n",
    "#     avg(x1,x2,x3,x4):    [x11+x21+x31+x41, x12+x22+x32+x42]/4,\n",
    "#     avg(x1,x2,x3,x4,x5): [x11+x21+x31+x41+x51, x12+x22+x32+x42+x52]/5\n",
    "# ]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b =\n",
      "tensor([[3., 0.],\n",
      "        [3., 2.],\n",
      "        [9., 4.]])\n",
      "--\n",
      "c =\n",
      "tensor([[3., 0.],\n",
      "        [3., 1.],\n",
      "        [5., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# the above is sound and good but inefficient, we want to find a way to do it with a matrix multiplication:\n",
    "# toy example illustrating how matrix multiplication can be used for a cumulative average over the stream of tokens\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True) # become \"weights\"\n",
    "b = torch.randint(0,10,(3,2)).float() # feature vectors\n",
    "c = a @ b\n",
    "print('a =')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b =')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c =')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# so let's vectorize the for-loop:\n",
    "weights = torch.tril(torch.ones(T,T)) \n",
    "weights = weights / torch.sum(weights, 1, keepdim=True)\n",
    "print(weights)\n",
    "\n",
    "xbow2 = weights @ x \n",
    "# (T,T) @ (B, T, C) thus pytorch broadcasts weights to make it (B,T,T) i.e. creates B (T,T) matrices and stacks them up \n",
    "# so we get (B,T,T) @ (B,T,C) = (B,T,C) <- which was the shape of x \n",
    "# so we have obtained a new repr of each token simple-averaging itself with its past\n",
    "# we can see this simple mean as a weighted sum where weights in this case are 1/n_past_tokens\n",
    "# we could use some more smart aggregation rules instead of simple-mean weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's rewrite the same thing as above with the softmax \n",
    "# why? -> see next cell\n",
    "tril = torch.tril(torch.ones(T,T)) \n",
    "weights = torch.zeros((T,T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # put -inf in all positions where the tril has 0s\n",
    "print(weights)\n",
    "\n",
    "# then we apply softmax over each row \n",
    "# e**0 = 1; e-inf = 0 so we replicate the exact weights from above\n",
    "weights = weights.softmax(-1) # 0 is over the cols, 1 over the rows\n",
    "print(weights)\n",
    "xbow3 = weights @ x\n",
    "\n",
    "torch.allclose(xbow, xbow2)\n",
    "torch.allclose(xbow, xbow3)\n",
    "torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why we want to use this form? \n",
    "\n",
    "# let's consider:\n",
    "weights = torch.zeros((T,T)) # \n",
    "# the weights start from a tensor of 0s, and the idea is that this vector (which has tril shape), represent the strenght of \n",
    "# the \"correlation\"/interaction-strenght/affinity\n",
    "# of each past token wrt current token. Or better it represent how much of each past token we want to consider/aggregate to be averaged up \n",
    "# in the computation of the context of the current token.\n",
    "\n",
    "# let's consider:\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # put -inf in all positions where the tril has 0s\n",
    "weights = weights.softmax(-1)\n",
    "# the idea here is that tokens from the future cannot interact/used/averaged to compute ctx of current token \n",
    "\n",
    "# the aggregation step\n",
    "xbow3 = weights @ x\n",
    "\n",
    "# the idea is that we are going to learn the cofficients of the weighted average, and these coefficients are going to be called \n",
    "# affinities or attention coeffs. These coeffs will be data dependant and will be defined by how much a each token is interested to other past tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention head/block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want gather information from the past in a data dependent way\n",
    "# how?\n",
    "# each token/node emits 2 vectors:\n",
    "# query: what am i looking for\n",
    "# keys: what do i contain\n",
    "\n",
    "# affinities: dot prod between keys and querys\n",
    "# so my query dotprod with all the keys of all the other tokens defines the weights matrix. The idea is that if the dot prod is high, then it means that the key matches the query \n",
    "# Example: if the query of tokenA has high dot prod with the key of a past tokenB, then the row of the weights/attention coeffs matrix where the tokenA is the last token considered\n",
    "# will define a high value for the idx-position of tokenB (nb the weights sum to 1)\n",
    "\n",
    "# single head perform self attention\n",
    "\n",
    "# set up fake data\n",
    "B, T, C = 4, 8, 32 # batch, tokens, token dimensionality\n",
    "x = torch.randn(B, T, C) # bs, ctx_len, token dimensionality \n",
    "\n",
    "# set up head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias = False) \n",
    "query = nn.Linear(C, head_size, bias = False) \n",
    "k = key(x) # (B, ctx_len, head_size) k: \"here is what I have\"\n",
    "q = query(x)# (B, ctx_len, head_size) q: \"here is my request/what i am interested in\"\n",
    "# each input token feature vector is used to create k,q. \n",
    "# each input token feature vector contains info on token identity and token position\n",
    "# so k,q are create wrt token identity and token position\n",
    "weights = k @ q.transpose(-2, -1) * head_size**-0.5 # k: (B, ctx_len, head_size) @  q: (B, head_size, ctx_len) --> (B, ctx_len, ctx_len) i.e. (B, T, T)\n",
    "# * head_size**-0.5: aka scaled attention. Idea: if weights not scaled, its variance is ~head_size -> when we apply softmax we might end up with a sharp pdist/~one-hot\n",
    "# which implies that we aggregate info from 1 single node/token which is bad: at init we want unormalized attention scores to be quite diffused \n",
    "\n",
    "tril = torch.tril(torch.ones(T,T)) \n",
    "# weights = torch.zeros((T,T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # put -inf in all positions where the tril has 0s\n",
    "# if we use an \"encoder\" block we delete the masking op here above: the idea is that eg if we want to do sentence classification (eg sentiment analysis) it\n",
    "# is not the case that we need to hide future tokens, cuz the algo \"works directly on the whole sentence\"\n",
    "# when we use the masking it is called a \"decoder\" block cuz is decoding language in this autoregressive manner \n",
    "\n",
    "# normalize attention scores\n",
    "weights = weights.softmax(-1) #F.softmax(weights, dim = -1) # softmax is always applied over the last dim\n",
    "\n",
    "# we don't aggregate directly raw x, but we get a version of x projected into a head_size dimensional space\n",
    "# out = weights @ x\n",
    "value = nn.Linear(C, head_size, bias = False) \n",
    "v = value(x) # v: \"here is what I communicate, my msg (if you find me interesting)\"\n",
    "out = weights @ v\n",
    "out.shape\n",
    "\n",
    "\n",
    "# you can thing to attention as a communication mechanism: you have N nodes and you can think as graphNN aggregation step with particular weight matrix \n",
    "# attention is position/space agnosting, it's us that we provide positional info by summing pos_embeddings to the input (not like cnns that are space aware)\n",
    "\n",
    "# IMPO: the attention above is \"self-attention\" cuz the k,q,v are all coming from the same input x\n",
    "# cross-attention is when  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5115, 0.4885, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2170, 0.5511, 0.2319, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2023, 0.2064, 0.2476, 0.3437, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1539, 0.2592, 0.1816, 0.2280, 0.1774, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2514, 0.1360, 0.1552, 0.1500, 0.1212, 0.1862, 0.0000, 0.0000],\n",
       "         [0.1071, 0.2490, 0.1200, 0.1561, 0.0963, 0.1064, 0.1651, 0.0000],\n",
       "         [0.1221, 0.1019, 0.0659, 0.1485, 0.1044, 0.1156, 0.1173, 0.2243]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6200, 0.3800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3634, 0.3304, 0.3062, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3164, 0.2543, 0.2079, 0.2213, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2759, 0.1713, 0.1802, 0.2785, 0.0942, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1189, 0.1927, 0.1843, 0.1472, 0.2079, 0.1491, 0.0000, 0.0000],\n",
       "         [0.1252, 0.1543, 0.1464, 0.1422, 0.1854, 0.1254, 0.1212, 0.0000],\n",
       "         [0.1520, 0.1062, 0.1163, 0.0733, 0.1224, 0.1178, 0.1148, 0.1973]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7224, 0.2776, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2640, 0.3768, 0.3592, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2258, 0.3877, 0.1309, 0.2556, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2037, 0.1491, 0.2948, 0.2022, 0.1503, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1105, 0.0980, 0.1654, 0.4310, 0.0768, 0.1182, 0.0000, 0.0000],\n",
       "         [0.1214, 0.1522, 0.0903, 0.1577, 0.1206, 0.2287, 0.1291, 0.0000],\n",
       "         [0.1075, 0.1289, 0.0725, 0.1480, 0.0940, 0.1141, 0.1047, 0.2303]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6175, 0.3825, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2036, 0.3841, 0.4123, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2430, 0.2184, 0.2755, 0.2632, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1864, 0.2851, 0.1135, 0.2544, 0.1606, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1364, 0.2083, 0.1257, 0.1605, 0.1515, 0.2176, 0.0000, 0.0000],\n",
       "         [0.1765, 0.1409, 0.0963, 0.1878, 0.1214, 0.1258, 0.1512, 0.0000],\n",
       "         [0.1741, 0.2116, 0.0369, 0.1585, 0.0653, 0.0692, 0.1232, 0.1611]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    '''\n",
    "    One single head of self attention\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_dims, out_dims, ctx_len):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(inp_dims, out_dims, bias=False)\n",
    "        self.query = nn.Linear(inp_dims, out_dims, bias=False)\n",
    "        self.value = nn.Linear(inp_dims, out_dims, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(ctx_len, ctx_len)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        # compute attention scores\n",
    "        weights = q @ k.transpose(-2,-1) * k.shape[-1] ** -0.5 # supposing k and q having the same dim\n",
    "        weights = weights.masked_fill(self.tril == 0, float('-inf')) # put -inf in all positions where the tril has 0s\n",
    "        weights = weights.softmax(-1)\n",
    "\n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = weights @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, T, C = 4, 8, 32 # batch, tokens, token dimensionality\n",
    "# x = torch.randn(B, T, C) # bs, ctx_len, token dimensionality \n",
    "# head = Head(C, 16, 8)\n",
    "# x = head(x)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    '''\n",
    "    The model learns each entry of a (vocab_len, vocab_len) table \n",
    "    where each entry is the probability dist of the following char given an input char at row\n",
    "    '''\n",
    "    def __init__(self, vocab_len, n_embed = 32):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table (bigram model lecture 2)    \n",
    "        self.token_embedding_table = nn.Embedding(vocab_len, n_embed)\n",
    "\n",
    "        # let's add a position embedding table, the idea is that we also want to embed the position of each token (and not onty its identity as usually done by older language models)\n",
    "        self.position_embedding_table = nn.Embedding(ctx_len, n_embed) # across our whole max context length we have other n_embed vectors\n",
    "        self.self_att_head = Head(n_embed, n_embed, ctx_len)\n",
    "        self.lang_model_head = nn.Linear(n_embed, vocab_len)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape # T is int:ctx_len \n",
    "        \n",
    "        # idx and targets are int tensors of shape (bs, ctx_len)\n",
    "        token_embeddings = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_embeddings = self.position_embedding_table(torch.arange(T)) # (T, n_embed)\n",
    "        x = token_embeddings + pos_embeddings # pos_embeddings broadcasted across batch dimension \n",
    "\n",
    "        x = self.self_att_head(x)\n",
    "\n",
    "        logits = self.lang_model_head(x) # (B, T, n_embed)\n",
    "        if targets == None: return logits, None        \n",
    "            \n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)                \n",
    "        return logits, loss\n",
    "    \n",
    "    # this function will change over the course of the lecture\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (bs, ctx_len) array of int-idxs that define the context\n",
    "        # these int-idxs are chars from the vocab\n",
    "        # in the bigram model only 1 char is looked at as ctx\n",
    "        for _ in range(max_new_tokens):\n",
    "            # new: crop idx to be the last ctx_size token\n",
    "            idx_cond = idx[:, -ctx_len:]\n",
    "            # predict i.e. get unnormalized probs\n",
    "            logits, _ = self(idx_cond)\n",
    "            # get last time step\n",
    "            logits = logits[:, -1, :] # (bs, out_classes), -1 cuz rn we are using only the last char in the bigram model\n",
    "            # normalize probs\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (bs,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (bs, T+1)\n",
    "        return idx\n",
    "    \n",
    "    # def generate(self, idx, max_new_tokens):\n",
    "    # # idx is (B, T) array of indices in the current context\n",
    "    #     for _ in range(max_new_tokens):\n",
    "    #         # crop idx to the last block_size tokens\n",
    "    #         idx_cond = idx[:, -ctx_len:]\n",
    "    #         # get the predictions\n",
    "    #         logits, _ = self(idx_cond)\n",
    "    #         # focus only on the last time step\n",
    "    #         logits = logits[:, -1, :] # becomes (B, C)\n",
    "    #         # apply softmax to get probabilities\n",
    "    #         probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "    #         # sample from the distribution\n",
    "    #         idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "    #         # append sampled index to the running sequence\n",
    "    #         idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    #     return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.135059356689453"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_len=vocab_len)\n",
    "optim = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = m(xb, yb)\n",
    "loss.item() # we know that the initial loss  must be -math.log(1/vocab_len) = 4.174387269895637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated train loss: 4.185403823852539, estimated val loss: 4.187540054321289\n",
      "Estimated train loss: 2.7183759212493896, estimated val loss: 2.744436740875244\n",
      "Estimated train loss: 2.584773063659668, estimated val loss: 2.5600268840789795\n",
      "Estimated train loss: 2.5059614181518555, estimated val loss: 2.5117766857147217\n",
      "Estimated train loss: 2.5088212490081787, estimated val loss: 2.5001912117004395\n",
      "Estimated train loss: 2.46691632270813, estimated val loss: 2.466158390045166\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_num_steps = 5001 #25001\n",
    "eval_interval = 1000\n",
    "for step in range(max_num_steps):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if step % eval_interval == 0:\n",
    "        out = estimate_loss(m)\n",
    "        print(f\"Estimated train loss: {out['train']}, estimated val loss: {out['val']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Whipecelre.\n",
      "\n",
      "ARHENoceiens hsel'dlle do!\n",
      "AUSon.\n",
      "\n",
      "N:\n",
      "CESNGEGE:\n",
      "pes ingm?\n",
      "\n",
      "Iin:\n",
      "A tathe jghintnge:\n",
      "Cat int re mbriont;\n",
      "Th ce sourn!\n",
      "Fior vrithepergy othepid myot.\n",
      "\n",
      "Cye plro le do?\n",
      "Ou\n",
      "s,\n",
      "Sovore nism\n",
      "IAned I it ther\n",
      "Ang ach, bet oengdrendee, tchausich ongor fitof mworu, tot, V:\n",
      "Ant ind Firt homt; thed, nde, we\n",
      "AGUCOUKENGI's.\n",
      "\n",
      "SI,\n",
      "Whe, thepreer st alu hoven bee hse thinliche ingtwharl thirt ible,\n",
      "Ft!l sleds y.\n",
      "TOe snd tht id tature imbemeect yot:\n",
      "easreich ot br merle ce'ded\n",
      "INEses, the ng ku hanet edsme,\n",
      "An dong ar ith, to l;\n",
      "Yhave byer'ghe whest, at hteent,\n",
      "O orust am\n",
      "W;Yasto lod\n",
      "G epcr cors hafre nd.\n",
      "\n",
      "ARYe rdelepin!r th\n",
      "Igf.\n",
      "\n",
      "Ph, othevogth,\n",
      "I;\n",
      "I fepree o hoin me; o ghee nath it he chay bon chon mangis, tsenconcs Mut, met do teiowis tcthant\n",
      "J; st, frfe onte sa'stel baureeangourus'ce wot sheds in ar rod ghe che,\n",
      "I.\n",
      "\n",
      "Hol yot'o,\n",
      "Metay tid--t lhedl oor;ine thetathe the hougte ne wivett he wivencofowiny cfet lis iest hedl.\n",
      "\n",
      "Tonoe tarsailousaveu lininld DI iur oruters, r whund,\n",
      "S-t douncovepixrgtrow\n",
      "\n",
      "CAn fse?\n",
      "\n",
      "Nnot II lando ng steangdlt ist haclorit,\n",
      "Bo llinsestobathiutorve omedicy hel at sarlve lelyof wounown grAn titus of las, fiteldyeche ono ilsf nt!ay ingenceru. Theiwo haten btfothealy, yofrsat, gh felor sus, hanoche chebease tetors: hade.\n",
      "\n",
      "I worel sye hand cor'th ow on, the py pe yom thy onk;\n",
      "Sh, vachert at te\n",
      "A wher or yored yeeas ancerd ur fonoky:\n",
      "I it, ss prdeld thanowo he nt sidoreser shead dt fotr, hyond RHrM orus\n",
      "Wheasesifthe cead binentt,\n",
      "MOH ees yore'r-:\n",
      "Hhen gher, it chithixree fm uest bdy yk to beay is tharerruburet, turt ml, pen otus ct,\n",
      "F worwive;\n",
      "Y hore cheing pronee cet, gikenatllolfrnor finu dganvaspofrhe\n",
      "Theen.\n",
      "Hurester.Yowhes be.\n",
      "\n",
      "LAle hy ancake hover oulseedat.\n",
      "\n",
      "Gow GLAt fangt ty otnst st oun wis fpre koond ads endad iculre ond thord:\n",
      "LO:\n",
      "OAr nariso m? Rileat otonrivak.\n",
      "\n",
      "Han.\n",
      "Yicher hse ly irk wier, uscher wer tONI\n",
      "Amise Y be sy louregt I sithpead te, woruse,'Y\n",
      "Angnt thel to heomes th, fod, tl tth, fm che l shad outry antis orse yowit wong hy anugen: ine\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 8), dtype=torch.long)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) ~1:16 \n",
    "2) restart from 1:22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathyAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
